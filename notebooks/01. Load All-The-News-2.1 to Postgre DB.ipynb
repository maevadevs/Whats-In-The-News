{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parallel-albany",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#About-The-Dataset\" data-toc-modified-id=\"About-The-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>About The Dataset</a></span></li><li><span><a href=\"#Preliminary-Analysis\" data-toc-modified-id=\"Preliminary-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preliminary Analysis</a></span></li><li><span><a href=\"#Import-Libraries-and-Set-Settings\" data-toc-modified-id=\"Import-Libraries-and-Set-Settings-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import Libraries and Set Settings</a></span></li><li><span><a href=\"#First-attempt-to-correct-the-problematic-entries-in-the-CSV\" data-toc-modified-id=\"First-attempt-to-correct-the-problematic-entries-in-the-CSV-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>First attempt to correct the problematic entries in the CSV</a></span></li><li><span><a href=\"#Loading-the-Data-into-the-Database\" data-toc-modified-id=\"Loading-the-Data-into-the-Database-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Loading the Data into the Database</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-exemption",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "- The purpose of this notebook is to load the contents of the `All-The-News-21.csv` dataset into Postgre SQL database as close to the raw source as possible\n",
    "- This dataset initially contains 2.7 million news articles and essays from 27 American publishers\n",
    "- We are only keeping those entries that do not have `NULL` in the `article` column\n",
    "- After some initial cleanups as done in this notebook, we end up with **2,584,165** article entries and 26 publishers loaded into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-trauma",
   "metadata": {},
   "source": [
    "## About The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-powder",
   "metadata": {},
   "source": [
    "- Name: All The News 2.1\n",
    "- Source: https://components.one/datasets/all-the-news-2-news-articles-dataset/\n",
    "\n",
    "This dataset contains 2.7 million news articles and essays from 27 American publications\n",
    "\n",
    "- Includes:\n",
    "  - date\n",
    "  - title\n",
    "  - publication\n",
    "  - article text\n",
    "  - publication name\n",
    "  - year\n",
    "  - month\n",
    "  - URL (for some)\n",
    "  \n",
    "Articles mostly span from 2013 to early 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-tongue",
   "metadata": {},
   "source": [
    "## Preliminary Analysis\n",
    "\n",
    "- After some initial cleanups done here, we end up with **2,584,165** article entries loaded into the DB\n",
    "- We skipped problematic entries that were not formatted correctly\n",
    "- The database now has the following table structure for the `AllTheNews21` table\n",
    "\n",
    "Columns | Data Type | Source | Has `NULL` or Empty Values | How many `NULL`/Empty Values? | Description\n",
    ":-|:-:|:-:|:-:|:-:|:-\n",
    "`index`|`bigint`|Original|No|N/A|Used as Primary Key\n",
    "`date`|`text`|Original|No|N/A|Date of publication of the news article\n",
    "`year`|`bigint`|Original|No|N/A|Year-part of the date of publication of the news article\n",
    "`month`|`bigint`|Original|No|N/A|Mnoth-part of the date of publication of the news article\n",
    "`day`|`bigint`|Original|No|N/A|Day-part of the date of publication of the news article\n",
    "`author`|`text`|Original|Yes|924,621|Author of the news article\n",
    "`title`|`text`|Original|Yes|16|Title of the news article\n",
    "`article`|`text`|Original|No|N/A|Content of the news article\n",
    "`url`|`text`|Original|No|N/A|URL of the news article\n",
    "`section`|`text`|Original|Yes|830,754|The section category of the news article\n",
    "`publication`|`text`|Original|No|N/A|The publisher of the news article\n",
    "`title_length`|`bigint`|Calculated|No|N/A|The string length of the title for the news article\n",
    "`article_length`|`bigint`|Calculated|No|N/A|The string length of the content for the news article\n",
    "\n",
    "**NOTES - TODOS**\n",
    "- **A lot of cleaning needs to be done for `section`: Approach would be to clean by Publication**\n",
    "- **`publication` is clean and contains 26 unique publishers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-oklahoma",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine   # conda install -c anaconda sqlalchemy\n",
    "from dotenv import load_dotenv         # conda install -c conda-forge python-dotenv\n",
    "import os                              # Python default package\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm         # Provides progress bar for long tasks\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "democratic-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-eight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # => True if no error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "established-cedar",
   "metadata": {},
   "source": [
    "# If needed, run these to verify that the settings from .env loaded correctly with the following lines\n",
    "# print(os.getenv(\"db_name\"))\n",
    "# print(os.getenv(\"db_username\"))\n",
    "# print(os.getenv(\"db_password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cross-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from the .env file\n",
    "db_name = os.getenv(\"db_name\")\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")\n",
    "connection_string = f\"postgres://{db_username}:{db_password}@localhost:5432/{db_name}\"\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-control",
   "metadata": {},
   "source": [
    "## First attempt to correct the problematic entries in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-lesson",
   "metadata": {},
   "source": [
    "This attempt did not work but I am keeping the codes in here for reference in case I need to go back to it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naval-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = open(\"datasets/all-the-news-2-1/parts/all-the-news-2-1_0.csv\", \"r\", encoding=\"utf8\").read().replace('\\n','')\n",
    "\n",
    "# with open(\"datasets/all-the-news-2-1/parts/all-the-news-2-1_0.csv__copy.csv\", \"w\", encoding=\"utf8\") as g:\n",
    "#     g.write(content)\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     \"datasets/all-the-news-2-1/parts/all-the-news-2-1_0.csv\", \n",
    "#     engine=\"python\", \n",
    "#     encoding=\"utf-8\",\n",
    "#     error_bad_lines=False,\n",
    "#     nrows=1000\n",
    "# )\n",
    "# print(df.shape)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "representative-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=[\"Unnamed: 0.1\"])\n",
    "# df = df.rename(columns={\"Unnamed: 0\": \"index\"})\n",
    "# print(df.shape)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "diverse-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"month\"] = df[\"month\"].astype(\"int64\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compound-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=['article'])\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-slave",
   "metadata": {},
   "source": [
    "## Loading the Data into the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-ireland",
   "metadata": {},
   "source": [
    "This is the section that works to load all the articles data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "normal-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell fixes the field larger than field limit (131072) Error that happens with some CSVs\n",
    "\n",
    "# Import libraries\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Settings\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "# Main Loop\n",
    "while True:\n",
    "    \n",
    "    # Decrease the maxInt value by factor 10 as long as the OverflowError occurs\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-agriculture",
   "metadata": {},
   "source": [
    "Now, loading the dataset into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "documented-bailey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096cf21b67af4f88be2a8655e3af98fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task done. ---\n"
     ]
    }
   ],
   "source": [
    "# All the CSV parts\n",
    "# parts_csv = glob.glob(\"datasets/all-the-news-2-1/parts/*.csv\")\n",
    "# \n",
    "# for part in parts_csv:\n",
    "#     print(\"\\n>>> Working on\", part, \"...\")\n",
    "\n",
    "with engine.connect() as db:\n",
    "    \n",
    "    # Import the whole csv\n",
    "    news_raw = pd.read_csv(\n",
    "        \"raw-datasets/all-the-news-21/original-massive-data/all-the-news-2-1.csv\", \n",
    "        engine=\"python\", \n",
    "        encoding=\"utf-8\",\n",
    "        error_bad_lines=False,\n",
    "        chunksize = 10000\n",
    "    )\n",
    "    \n",
    "    # Looping over chuncks of news_raw\n",
    "    for chunk in tqdm(news_raw):\n",
    "        \n",
    "        # Remove/Rename some columns\n",
    "        chunk = chunk.drop(columns=[\"Unnamed: 0.1\"])\n",
    "        chunk = chunk.rename(columns={\"Unnamed: 0\": \"index\"})\n",
    "\n",
    "        # Drop the record if it does not have a text\n",
    "        chunk = chunk.dropna(subset=['article'])\n",
    "        \n",
    "        # Append the chunk to the table\n",
    "        chunk.to_sql(\n",
    "            'AllTheNews21', # The table name\n",
    "            db, # The database\n",
    "            if_exists = 'append', \n",
    "            index = False # Do not include the pandas index column as PK\n",
    "        )\n",
    "\n",
    "    # When all is done, print done\n",
    "    print('--- Task done. ---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infinite-community",
   "metadata": {},
   "source": [
    "# Load BBC to Postgre DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-wagon",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Import-Libraries-and-Settings\" data-toc-modified-id=\"Import-Libraries-and-Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Libraries and Settings</a></span></li><li><span><a href=\"#Read-Text-Files-Into-Dataframe\" data-toc-modified-id=\"Read-Text-Files-Into-Dataframe-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Read Text Files Into Dataframe</a></span></li><li><span><a href=\"#Export-to-CSV\" data-toc-modified-id=\"Export-to-CSV-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Export to CSV</a></span></li><li><span><a href=\"#Load-into-DB\" data-toc-modified-id=\"Load-into-DB-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Load into DB</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-walker",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "- The purpose of this notebook is to load the contents of the BBC and the BBCSport datasets into Postgre SQL database as close to the raw source as possible\n",
    "- The **BBC dataset** consists of **2225 documents** from the BBC news website corresponding to stories in five topical areas from **2004-2005**.\n",
    "  - business\n",
    "  - entertainment\n",
    "  - politics\n",
    "  - sport\n",
    "  - tech\n",
    "- The **BBCSport dataset** consists of **737 documents** from the BBC Sport website corresponding to sports news articles in five topical areas from **2004-2005**.\n",
    "  - athletics\n",
    "  - cricket\n",
    "  - football\n",
    "  - rugby\n",
    "  - tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-playback",
   "metadata": {},
   "source": [
    "## Import Libraries and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine   # conda install -c anaconda sqlalchemy\n",
    "from dotenv import load_dotenv         # conda install -c conda-forge python-dotenv\n",
    "import os                              # Python default package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm         # Provides progress bar for long tasks\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standing-westminster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # => True if no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generous-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from the .env file\n",
    "db_name = os.getenv(\"db_name\")\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")\n",
    "connection_string = f\"postgres://{db_username}:{db_password}@localhost:5432/{db_name}\"\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-greensboro",
   "metadata": {},
   "source": [
    "## Read Text Files Into Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-lodging",
   "metadata": {},
   "source": [
    "**Only run this if restarting from raw data. This line takes long to run. Instead, use the exported `bbc.csv` to reset the DB**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "electronic-superintendent",
   "metadata": {},
   "source": [
    "# Grab the path of each file and store in a list\n",
    "filepaths = np.array(glob.glob('../raw-datasets/bbc-datasets/bbc/bbc-fulltext/**/*', recursive = True))\n",
    "\n",
    "# Filter out the first 6: Root paths\n",
    "filepaths = filepaths[6:]\n",
    "\n",
    "# Create a dataframe\n",
    "bbc_df = pd.DataFrame(list(np.char.split(filepaths, sep=\"\\\\\")), columns=['path', 'category', 'document'])\n",
    "bbc_df = bbc_df.drop(columns=[\"path\"])\n",
    "\n",
    "# Read the contents of each file and finalize the dataframe\n",
    "all_titles = np.array([])\n",
    "all_contents = np.array([])\n",
    "\n",
    "for path in filepaths:\n",
    "    \n",
    "    # Open the file and parse through the title and the contents\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        title = lines[0].replace('\\n', '')\n",
    "        contents = \"\".join(lines[1:]).replace('\\n', '')\n",
    "    \n",
    "        # Append the values\n",
    "        all_titles = np.append(all_titles, title)\n",
    "        all_contents = np.append(all_contents, contents)\n",
    "\n",
    "bbc_df[\"titles\"] = all_titles\n",
    "bbc_df[\"contents\"] = all_contents\n",
    "\n",
    "# Finally, drop the document column as we only used it to grab each file\n",
    "bbc_df = bbc_df.drop(columns=[\"document\"])\n",
    "\n",
    "# Sneak-peek\n",
    "display(bbc_df.shape)\n",
    "display(bbc_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-sword",
   "metadata": {},
   "source": [
    "Let's do the same for the BBC Sport dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "statutory-causing",
   "metadata": {},
   "source": [
    "# Grab the path of each file and store in a list\n",
    "filepaths = np.array(glob.glob('../raw-datasets/bbc-datasets/bbcsport/bbcsport-fulltext/**/*', recursive = True))\n",
    "\n",
    "# Filter out the first 6: Root paths\n",
    "filepaths = filepaths[6:]\n",
    "\n",
    "# Create a dataframe\n",
    "bbcsports_df = pd.DataFrame(list(np.char.split(filepaths, sep=\"\\\\\")), columns=['path', 'category', 'document'])\n",
    "bbcsports_df = bbcsports_df.drop(columns=[\"path\"])\n",
    "\n",
    "# Read the contents of each file and finalize the dataframe\n",
    "all_titles = np.array([])\n",
    "all_contents = np.array([])\n",
    "\n",
    "for path in filepaths:\n",
    "    \n",
    "    # Open the file and parse through the title and the contents\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        title = lines[0].replace('\\n', '')\n",
    "        contents = \"\".join(lines[1:]).replace('\\n', '')\n",
    "    \n",
    "        # Append the values\n",
    "        all_titles = np.append(all_titles, title)\n",
    "        all_contents = np.append(all_contents, contents)\n",
    "\n",
    "bbcsports_df[\"titles\"] = all_titles\n",
    "bbcsports_df[\"contents\"] = all_contents\n",
    "\n",
    "# Finally, drop the document column as we only used it to grab each file\n",
    "bbcsports_df = bbcsports_df.drop(columns=[\"document\"])\n",
    "\n",
    "# Sneak-peek\n",
    "display(bbcsports_df.shape)\n",
    "display(bbcsports_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-bristol",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-account",
   "metadata": {},
   "source": [
    "**Only run this if restarting from raw data. This line takes long to run. Instead, use the exported `bbc.csv` to reset the DB**\n",
    "\n",
    "Let's export this into a CSV file to make it easier to use later in case we need it again later"
   ]
  },
  {
   "cell_type": "raw",
   "id": "random-diagram",
   "metadata": {},
   "source": [
    "bbc_df.to_csv(\"../clean-datasets/bbc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "optical-multiple",
   "metadata": {},
   "source": [
    "bbcsports_df.to_csv(\"../clean-datasets/bbcsports.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-mounting",
   "metadata": {},
   "source": [
    "## Load into DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-sellers",
   "metadata": {},
   "source": [
    "**If re-running this, make sure to drop the table first**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "generous-relaxation",
   "metadata": {},
   "source": [
    "# Loading the BBC dataset into DB --> BBCArticles\n",
    "with engine.connect() as db:\n",
    "    \n",
    "    # Import the whole csv\n",
    "    news_raw = pd.read_csv(\n",
    "        \"../clean-datasets/bbc.csv\", \n",
    "        engine=\"python\", \n",
    "        encoding=\"utf-8\",\n",
    "        error_bad_lines=False,\n",
    "        chunksize = 1000\n",
    "    )\n",
    "    \n",
    "    # Looping over chuncks of news_raw\n",
    "    for chunk in tqdm(news_raw):\n",
    "        \n",
    "        # Append the chunk to the table\n",
    "        chunk.to_sql(\n",
    "            'BBCArticles', # The table name\n",
    "            db, # The database\n",
    "            if_exists = 'append', \n",
    "            index = False # Do not include the pandas index column as PK\n",
    "        )\n",
    "\n",
    "    # When all is done, print done\n",
    "    print('--- Task done. ---')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mediterranean-action",
   "metadata": {},
   "source": [
    "# Loading the BBC Sports dataset into DB --> BBCSportsArticles\n",
    "with engine.connect() as db:\n",
    "    \n",
    "    # Import the whole csv\n",
    "    news_raw = pd.read_csv(\n",
    "        \"../clean-datasets/bbcsports.csv\", \n",
    "        engine=\"python\", \n",
    "        encoding=\"utf-8\",\n",
    "        error_bad_lines=False,\n",
    "        chunksize = 1000\n",
    "    )\n",
    "    \n",
    "    # Looping over chuncks of news_raw\n",
    "    for chunk in tqdm(news_raw):\n",
    "        \n",
    "        # Append the chunk to the table\n",
    "        chunk.to_sql(\n",
    "            'BBCSportsArticles', # The table name\n",
    "            db, # The database\n",
    "            if_exists = 'append', \n",
    "            index = False # Do not include the pandas index column as PK\n",
    "        )\n",
    "\n",
    "    # When all is done, print done\n",
    "    print('--- Task done. ---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "formed-discussion",
   "metadata": {},
   "source": [
    "# Classification -- BBC Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-mission",
   "metadata": {},
   "source": [
    "- Text Vectorization:\n",
    "  - TF-IDF Vectorizer from scikit-learn\n",
    "- Classifier:\n",
    "  - Random Forest\n",
    "  - Logistic Regression\n",
    "  - K-Nearest Neighbors\n",
    "  - Simple Decision Tree\n",
    "  - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-eclipse",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries-and-Set-Settings\" data-toc-modified-id=\"Import-Libraries-and-Set-Settings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries and Set Settings</a></span></li><li><span><a href=\"#Import-Dataset\" data-toc-modified-id=\"Import-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Dataset</a></span></li><li><span><a href=\"#EDA-&amp;-Feature-Engineering\" data-toc-modified-id=\"EDA-&amp;-Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EDA &amp; Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-many-unique-classes-in-the-category?\" data-toc-modified-id=\"How-many-unique-classes-in-the-category?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>How many unique classes in the category?</a></span></li><li><span><a href=\"#How-many-rows-and-how-many-columns?\" data-toc-modified-id=\"How-many-rows-and-how-many-columns?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>How many rows and how many columns?</a></span></li><li><span><a href=\"#What-are-the-data-types-of-each-columns?\" data-toc-modified-id=\"What-are-the-data-types-of-each-columns?-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>What are the data types of each columns?</a></span></li><li><span><a href=\"#Do-we-have-any-missing-values?\" data-toc-modified-id=\"Do-we-have-any-missing-values?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Do we have any missing values?</a></span></li><li><span><a href=\"#Is-the-target-variable-balanced-or-not?\" data-toc-modified-id=\"Is-the-target-variable-balanced-or-not?-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Is the target variable balanced or not?</a></span></li><li><span><a href=\"#Feature-Engineering:-Text-Length\" data-toc-modified-id=\"Feature-Engineering:-Text-Length-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Feature Engineering: Text Length</a></span></li></ul></li><li><span><a href=\"#Cleaning-And-Pre-Processing\" data-toc-modified-id=\"Cleaning-And-Pre-Processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Cleaning And Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Wordcloud---What-words-are-most-frequent-for-each-category?\" data-toc-modified-id=\"Wordcloud---What-words-are-most-frequent-for-each-category?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Wordcloud - What words are most frequent for each category?</a></span></li></ul></li><li><span><a href=\"#Label-Encoding\" data-toc-modified-id=\"Label-Encoding-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Label Encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-are-these-labels-mapped?\" data-toc-modified-id=\"How-are-these-labels-mapped?-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>How are these labels mapped?</a></span></li></ul></li><li><span><a href=\"#Classification----Ground-Work\" data-toc-modified-id=\"Classification----Ground-Work-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Classification -- Ground Work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-data-to-Training-And-Testing\" data-toc-modified-id=\"Split-data-to-Training-And-Testing-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Split data to Training And Testing</a></span></li><li><span><a href=\"#Using-TF-IDF-for-Vectorizing-into-Word-Embedding\" data-toc-modified-id=\"Using-TF-IDF-for-Vectorizing-into-Word-Embedding-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Using TF-IDF for Vectorizing into Word Embedding</a></span></li></ul></li><li><span><a href=\"#Classification:-Using-Random-Forest-Model-(RF)\" data-toc-modified-id=\"Classification:-Using-Random-Forest-Model-(RF)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Classification: Using Random Forest Model (RF)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-Performance-Metrics\" data-toc-modified-id=\"Random-Forest-Performance-Metrics-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Random Forest Performance Metrics</a></span></li></ul></li><li><span><a href=\"#Classification:-Using-Logistic-Regression-Model-(LR)\" data-toc-modified-id=\"Classification:-Using-Logistic-Regression-Model-(LR)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Classification: Using Logistic Regression Model (LR)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression-Performance-Metrics\" data-toc-modified-id=\"Logistic-Regression-Performance-Metrics-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Logistic Regression Performance Metrics</a></span></li></ul></li><li><span><a href=\"#Classification:-Using-K-Nearest-Neighbors-Model-(KNN)\" data-toc-modified-id=\"Classification:-Using-K-Nearest-Neighbors-Model-(KNN)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Classification: Using K-Nearest Neighbors Model (KNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbors-Performance-Metrics\" data-toc-modified-id=\"K-Nearest-Neighbors-Performance-Metrics-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>K-Nearest Neighbors Performance Metrics</a></span></li></ul></li><li><span><a href=\"#Classification:-Using-Simple-Decision-Tree-Model-(dt)\" data-toc-modified-id=\"Classification:-Using-Simple-Decision-Tree-Model-(dt)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Classification: Using Simple Decision Tree Model (dt)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Decision-Tree-Performance-Metrics\" data-toc-modified-id=\"Simple-Decision-Tree-Performance-Metrics-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Simple Decision Tree Performance Metrics</a></span></li></ul></li><li><span><a href=\"#Classification:-Using-Gaussian-Naive-Bayes-Model-(gnb)\" data-toc-modified-id=\"Classification:-Using-Gaussian-Naive-Bayes-Model-(gnb)-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Classification: Using Gaussian Naive Bayes Model (gnb)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gaussian-Naive-Bayes-Performance-Metrics\" data-toc-modified-id=\"Gaussian-Naive-Bayes-Performance-Metrics-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Gaussian Naive Bayes Performance Metrics</a></span></li></ul></li><li><span><a href=\"#Saving-A-Trained-Model\" data-toc-modified-id=\"Saving-A-Trained-Model-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Saving A Trained Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-pickle\" data-toc-modified-id=\"Using-pickle-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Using <code>pickle</code></a></span></li><li><span><a href=\"#Using-joblib\" data-toc-modified-id=\"Using-joblib-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Using <code>joblib</code></a></span></li></ul></li><li><span><a href=\"#Hyper-Parameters-Tuning\" data-toc-modified-id=\"Hyper-Parameters-Tuning-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Hyper-Parameters Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Random-Forest\" data-toc-modified-id=\"Using-Random-Forest-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Using Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-GridSearch\" data-toc-modified-id=\"Using-GridSearch-13.1.1\"><span class=\"toc-item-num\">13.1.1&nbsp;&nbsp;</span>Using GridSearch</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-bulgaria",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "directed-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                              # Python default package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sqlalchemy import create_engine   # conda install -c anaconda sqlalchemy\n",
    "from dotenv import load_dotenv         # conda install -c conda-forge python-dotenv\n",
    "from wordcloud import WordCloud        # conda install -c conda-forge wordcloud\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raising-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tracked-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suspected-envelope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # => True if no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "formal-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from the .env file\n",
    "db_name = os.getenv(\"db_name\")\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")\n",
    "db_table_schema = os.getenv(\"db_table_schema\")\n",
    "connection_string = f\"postgres://{db_username}:{db_password}@localhost:5432/{db_name}\"\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "biblical-increase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AllTheNews21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBCArticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BBCSportsArticles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          table_name\n",
       "0       AllTheNews21\n",
       "1        BBCArticles\n",
       "2  BBCSportsArticles"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of available tables in the DB\n",
    "q = \"\"\"\n",
    "SELECT * \n",
    "FROM information_schema.tables\n",
    "WHERE table_catalog = '{db_name}'\n",
    "AND table_schema = '{db_table_schema}';\n",
    "\"\"\".format(\n",
    "    db_name = db_name,\n",
    "    db_table_schema = db_table_schema\n",
    ")\n",
    "\n",
    "pd.read_sql(q, con=engine)[[\"table_name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-borough",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-heath",
   "metadata": {},
   "source": [
    "In this notebook, we will be testing on `BBCArticles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alien-triangle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Share boost for feud-hit Reliance</td>\n",
       "      <td>The board of Indian conglomerate Reliance has ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                             titles  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Share boost for feud-hit Reliance   \n",
       "\n",
       "                                            contents  \n",
       "0  Quarterly profits at US media giant TimeWarner...  \n",
       "1  The dollar has hit its highest level against t...  \n",
       "2  The owners of embattled Russian oil giant Yuko...  \n",
       "3  British Airways has blamed high fuel prices fo...  \n",
       "4  The board of Indian conglomerate Reliance has ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "SELECT *\n",
    "FROM public.\"BBCArticles\";\n",
    "\"\"\"\n",
    "bbc = pd.read_sql(q, con=engine)\n",
    "\n",
    "display(bbc.shape)\n",
    "display(bbc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-photography",
   "metadata": {},
   "source": [
    "- Our *Predictor* is `contents`\n",
    "- Our *Target* is `category`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-packing",
   "metadata": {},
   "source": [
    "## EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-jacket",
   "metadata": {},
   "source": [
    "### How many unique classes in the category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "careful-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'sport', 'politics', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-importance",
   "metadata": {},
   "source": [
    "### How many rows and how many columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minimal-tourist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-deficit",
   "metadata": {},
   "source": [
    "### What are the data types of each columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "complicated-phrase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    object\n",
       "titles      object\n",
       "contents    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-airplane",
   "metadata": {},
   "source": [
    "### Do we have any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "million-cornwall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    False\n",
       "titles      False\n",
       "contents    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-technician",
   "metadata": {},
   "source": [
    "### Is the target variable balanced or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minus-maximum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = bbc[\"category\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-perfume",
   "metadata": {},
   "source": [
    "### Feature Engineering: Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hidden-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>content_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Share boost for feud-hit Reliance</td>\n",
       "      <td>The board of Indian conglomerate Reliance has ...</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                             titles  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Share boost for feud-hit Reliance   \n",
       "\n",
       "                                            contents  content_lengths  \n",
       "0  Quarterly profits at US media giant TimeWarner...             2516  \n",
       "1  The dollar has hit its highest level against t...             2213  \n",
       "2  The owners of embattled Russian oil giant Yuko...             1512  \n",
       "3  British Airways has blamed high fuel prices fo...             2368  \n",
       "4  The board of Indian conglomerate Reliance has ...              849  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc[\"content_lengths\"] = bbc[\"contents\"].str.len()\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "corporate-average",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x = bbc[\"content_lengths\"]).set_title(\"News Content Lengths Distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-respondent",
   "metadata": {},
   "source": [
    "## Cleaning And Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interested-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "looking-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing function\n",
    "def process_text(text):\n",
    "    \"\"\"Preprocess a given text: \n",
    "        - Tokenize\n",
    "        - Remove non-needed tokens\n",
    "        - Clean\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase, replace newlines with spaces, strip whitespaces\n",
    "    text = text.lower().replace(\"\\n\", \" \").replace(\"\\r\", \"\").strip()\n",
    "    \n",
    "    # Delete multiple spaces to only one space\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    \n",
    "    # Only keep actual words (Remove punctuations)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "    # Convert into a setence form\n",
    "    sentence = \" \".join(word_tokens)\n",
    "    \n",
    "    # Return final tokens\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reflected-bennett",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>content_lengths</th>\n",
       "      <th>processed_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>2516</td>\n",
       "      <td>quarterly profits us media giant timewarner ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>2213</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>1512</td>\n",
       "      <td>owners embattled russian oil giant yukos ask b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>2368</td>\n",
       "      <td>british airways blamed high fuel prices 40 dro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Share boost for feud-hit Reliance</td>\n",
       "      <td>The board of Indian conglomerate Reliance has ...</td>\n",
       "      <td>849</td>\n",
       "      <td>board indian conglomerate reliance agreed shar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                             titles  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Share boost for feud-hit Reliance   \n",
       "\n",
       "                                            contents  content_lengths  \\\n",
       "0  Quarterly profits at US media giant TimeWarner...             2516   \n",
       "1  The dollar has hit its highest level against t...             2213   \n",
       "2  The owners of embattled Russian oil giant Yuko...             1512   \n",
       "3  British Airways has blamed high fuel prices fo...             2368   \n",
       "4  The board of Indian conglomerate Reliance has ...              849   \n",
       "\n",
       "                                  processed_contents  \n",
       "0  quarterly profits us media giant timewarner ju...  \n",
       "1  dollar hit highest level euro almost three mon...  \n",
       "2  owners embattled russian oil giant yukos ask b...  \n",
       "3  british airways blamed high fuel prices 40 dro...  \n",
       "4  board indian conglomerate reliance agreed shar...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Preprocessing on the dataframe\n",
    "bbc[\"processed_contents\"] = bbc[\"contents\"].apply(process_text)\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-dependence",
   "metadata": {},
   "source": [
    "### Wordcloud - What words are most frequent for each category?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-analyst",
   "metadata": {},
   "source": [
    "Using wordcloud to visualize the words in the preprocessed texts, per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "resistant-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0771d049e8c24456b78e3194247879ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Category', options=('business', 'entertainment', 'sport', 'politic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of options for categories\n",
    "category_options = list(bbc[\"category\"].unique())\n",
    "\n",
    "# Create the widget andler for category\n",
    "widget = widgets.Dropdown(\n",
    "    options=category_options,   # The list of available options\n",
    "    index=0,                    # The index of the default selection\n",
    "    value=category_options[0],  # The value of the default selection\n",
    "    label=category_options[0],  # The label corresponding to the selected value\n",
    "    disabled=False,             # Whether to disable user changes\n",
    "    description='Category'      # Label\n",
    ")\n",
    "\n",
    "# Use the widget on the wordcloud function\n",
    "@interact(\n",
    "    # Specifying the handler for the category argument in the function\n",
    "    category=widget,\n",
    "    df=fixed(bbc)\n",
    ")\n",
    "def create_wordcloud(df, category):\n",
    "    \"\"\"Create a wordcloud visualization for a category of articles\"\"\"\n",
    "    \n",
    "    # Subset the original dataframe to only the category to consider\n",
    "    subset = df[df[\"category\"] == category]\n",
    "    \n",
    "    # Get all the texts of the subset\n",
    "    text = subset[\"processed_contents\"].values\n",
    "    \n",
    "    # Combine all the texts into a big single string\n",
    "    words = \" \".join(text)\n",
    "    \n",
    "    # Generate a WordCoud object\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"#fff\",  # the background color is white \n",
    "        height=500,               # the height is set to 500 \n",
    "        width=800,                # set the width to 800 \n",
    "        scale=20,                 # length-width stretching degree is set to 20 \n",
    "        prefer_horizontal=0.2,    # the adjustment level shows a tendency of 0.2\n",
    "        max_words=1000,           # set the maximum display word count to 1000 \n",
    "        relative_scaling=0.3,     # set the correlation between font size and word frequency to 0.3\n",
    "        max_font_size=120,        # reduce the maximum font size to 120\n",
    "        random_state=21,          # For reproducing steps\n",
    "        colormap=\"Set2\"           # Matplotlib Colormap\n",
    "    ).generate(words)\n",
    "    \n",
    "    # Plot the WordCloud instance\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-charles",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-jason",
   "metadata": {},
   "source": [
    "Converting the text of the news articles into numerical features for doing machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "consistent-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "smooth-judge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>content_lengths</th>\n",
       "      <th>processed_contents</th>\n",
       "      <th>category_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>2516</td>\n",
       "      <td>quarterly profits us media giant timewarner ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>2213</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>1512</td>\n",
       "      <td>owners embattled russian oil giant yukos ask b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>2368</td>\n",
       "      <td>british airways blamed high fuel prices 40 dro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Share boost for feud-hit Reliance</td>\n",
       "      <td>The board of Indian conglomerate Reliance has ...</td>\n",
       "      <td>849</td>\n",
       "      <td>board indian conglomerate reliance agreed shar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                             titles  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Share boost for feud-hit Reliance   \n",
       "\n",
       "                                            contents  content_lengths  \\\n",
       "0  Quarterly profits at US media giant TimeWarner...             2516   \n",
       "1  The dollar has hit its highest level against t...             2213   \n",
       "2  The owners of embattled Russian oil giant Yuko...             1512   \n",
       "3  British Airways has blamed high fuel prices fo...             2368   \n",
       "4  The board of Indian conglomerate Reliance has ...              849   \n",
       "\n",
       "                                  processed_contents  category_target  \n",
       "0  quarterly profits us media giant timewarner ju...                0  \n",
       "1  dollar hit highest level euro almost three mon...                0  \n",
       "2  owners embattled russian oil giant yukos ask b...                0  \n",
       "3  british airways blamed high fuel prices 40 dro...                0  \n",
       "4  board indian conglomerate reliance agreed shar...                0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit-Transform the Category feature\n",
    "bbc[\"category_target\"] = label_encoder.fit_transform(bbc[\"category\"])\n",
    "\n",
    "# Check the result\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "circular-trial",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 2, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc[\"category_target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-provider",
   "metadata": {},
   "source": [
    "**Let's export to CSV at this stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coral-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc.to_csv(\"../clean-datasets/bbc-with-category-target.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-breathing",
   "metadata": {},
   "source": [
    "### How are these labels mapped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grateful-internet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('business', 0)\n",
      "('entertainment', 1)\n",
      "('politics', 2)\n",
      "('sport', 3)\n",
      "('tech', 4)\n"
     ]
    }
   ],
   "source": [
    "# Each pair of (category, category_target) records\n",
    "records = bbc[[\"category\", \"category_target\"]].to_records(index=False)\n",
    "\n",
    "# Checking the mapping of the new labels\n",
    "for pair in np.unique(np.array(records)):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-permission",
   "metadata": {},
   "source": [
    "## Classification -- Ground Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coupled-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>content_lengths</th>\n",
       "      <th>processed_contents</th>\n",
       "      <th>category_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>2516</td>\n",
       "      <td>quarterly profits us media giant timewarner ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>2213</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>1512</td>\n",
       "      <td>owners embattled russian oil giant yukos ask b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>2368</td>\n",
       "      <td>british airways blamed high fuel prices 40 dro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Share boost for feud-hit Reliance</td>\n",
       "      <td>The board of Indian conglomerate Reliance has ...</td>\n",
       "      <td>849</td>\n",
       "      <td>board indian conglomerate reliance agreed shar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                             titles  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Share boost for feud-hit Reliance   \n",
       "\n",
       "                                            contents  content_lengths  \\\n",
       "0  Quarterly profits at US media giant TimeWarner...             2516   \n",
       "1  The dollar has hit its highest level against t...             2213   \n",
       "2  The owners of embattled Russian oil giant Yuko...             1512   \n",
       "3  British Airways has blamed high fuel prices fo...             2368   \n",
       "4  The board of Indian conglomerate Reliance has ...              849   \n",
       "\n",
       "                                  processed_contents  category_target  \n",
       "0  quarterly profits us media giant timewarner ju...                0  \n",
       "1  dollar hit highest level euro almost three mon...                0  \n",
       "2  owners embattled russian oil giant yukos ask b...                0  \n",
       "3  british airways blamed high fuel prices 40 dro...                0  \n",
       "4  board indian conglomerate reliance agreed shar...                0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "bbc = pd.read_csv(\"../clean-datasets/bbc-with-category-target.csv\")\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-vocabulary",
   "metadata": {},
   "source": [
    "### Split data to Training And Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "challenging-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "municipal-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Target\n",
    "X = bbc[\"processed_contents\"]\n",
    "y = bbc[\"category_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "altered-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=777,\n",
    "    stratify=y # Make sure to have the target column evenly distributed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "special-parameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2225,)\n",
      "X_train: (1780,)\n",
      "X_test: (445,)\n"
     ]
    }
   ],
   "source": [
    "# Check result of splitting\n",
    "print(\"X:\", X.shape)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-island",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- **Currently, all the X's are in text format**\n",
    "- **We need to convert them into vectorial format instead**\n",
    "- **This will be done using TF-IDF Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-ivory",
   "metadata": {},
   "source": [
    "### Using TF-IDF for Vectorizing into Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-loading",
   "metadata": {},
   "source": [
    "**TODO: Test Other ideas: fast-text or word2vec from gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dimensional-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-error",
   "metadata": {},
   "source": [
    "- Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "- `TfidfVectorizer` is equivalent to `CountVectorizer` followed by `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-therapist",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    *,\n",
    "    input='content', \n",
    "    encoding='utf-8', \n",
    "    decode_error='strict', \n",
    "    strip_accents=None, \n",
    "    lowercase=True, \n",
    "    preprocessor=None, \n",
    "    tokenizer=None, \n",
    "    analyzer='word', \n",
    "    stop_words=None, \n",
    "    token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "    ngram_range=(1, 1), \n",
    "    max_df=1.0, \n",
    "    min_df=1, \n",
    "    max_features=None, \n",
    "    vocabulary=None, \n",
    "    binary=False, \n",
    "    dtype=<class 'numpy.float64'>, \n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-tsunami",
   "metadata": {},
   "source": [
    "Define the hyperparameters of the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "outstanding-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for TF-IDF Vectorizer\n",
    "ngram_range = (1, 2)   # We are doing up to bigrams\n",
    "min_df = 10            # When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature\n",
    "max_df = 1.0           # When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
    "# max_features = 300\n",
    "norm = \"l2\"\n",
    "sublinear_tf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hungry-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer instance\n",
    "tfidf = TfidfVectorizer(\n",
    "    encoding=\"utf-8\",\n",
    "    ngram_range=ngram_range,\n",
    "    stop_words=None, # We already cleaned the text earlier in pre-processing\n",
    "    lowercase=False, # We already cleaned the text earlier in pre-processing\n",
    "    max_df=max_df,\n",
    "    min_df=min_df,\n",
    "#     max_features=max_features,\n",
    "    norm=norm,\n",
    "    sublinear_tf=sublinear_tf\n",
    ")\n",
    "\n",
    "# Store the vectorized features and labels of the training and testing data\n",
    "# We can pass these to various ML algorithms later for comparing classification performance\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "# Only transform on the test data\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abstract-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train: (1780, 5765)\n",
      "labels_train: (1780,)\n",
      "features_test: (445, 5765)\n",
      "labels_test: (445,)\n"
     ]
    }
   ],
   "source": [
    "# Checking where we are\n",
    "print(\"features_train:\", features_train.shape)\n",
    "print(\"labels_train:\", labels_train.shape)\n",
    "print(\"features_test:\", features_test.shape)\n",
    "print(\"labels_test:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "portuguese-gates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vectorized Features for Training:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--- Target Labels:\n",
      "709     1\n",
      "1374    3\n",
      "2016    4\n",
      "704     1\n",
      "260     0\n",
      "1709    3\n",
      "2162    4\n",
      "793     1\n",
      "56      0\n",
      "1342    3\n",
      "2079    4\n",
      "2013    4\n",
      "826     1\n",
      "1857    4\n",
      "139     1\n",
      "1727    3\n",
      "329     0\n",
      "1323    2\n",
      "686     1\n",
      "1793    3\n",
      "643     1\n",
      "1992    4\n",
      "1903    4\n",
      "844     1\n",
      "1914    4\n",
      "781     1\n",
      "1890    4\n",
      "2189    4\n",
      "368     0\n",
      "642     1\n",
      "1231    3\n",
      "1980    4\n",
      "1911    4\n",
      "2042    4\n",
      "2173    4\n",
      "1505    3\n",
      "836     1\n",
      "488     0\n",
      "787     1\n",
      "791     1\n",
      "1366    3\n",
      "1215    2\n",
      "233     0\n",
      "1414    3\n",
      "142     0\n",
      "834     1\n",
      "1332    2\n",
      "1735    3\n",
      "1961    4\n",
      "215     0\n",
      "1532    3\n",
      "1476    3\n",
      "2208    4\n",
      "1378    3\n",
      "68      0\n",
      "1781    3\n",
      "145     0\n",
      "2095    4\n",
      "1831    4\n",
      "255     0\n",
      "1481    3\n",
      "2207    4\n",
      "185     0\n",
      "1240    2\n",
      "2018    4\n",
      "83      0\n",
      "2144    4\n",
      "1477    3\n",
      "1712    3\n",
      "1027    2\n",
      "850     1\n",
      "1872    4\n",
      "113     0\n",
      "1399    3\n",
      "1670    3\n",
      "1492    3\n",
      "405     0\n",
      "1962    4\n",
      "1189    2\n",
      "1546    3\n",
      "213     0\n",
      "2082    4\n",
      "2011    4\n",
      "1282    2\n",
      "1460    3\n",
      "2134    4\n",
      "136     0\n",
      "888     1\n",
      "725     1\n",
      "1779    3\n",
      "974     2\n",
      "1731    3\n",
      "561     1\n",
      "1088    2\n",
      "102     0\n",
      "524     0\n",
      "1584    3\n",
      "515     0\n",
      "1433    3\n",
      "1084    2\n",
      "1091    2\n",
      "120     0\n",
      "1404    3\n",
      "206     0\n",
      "739     1\n",
      "349     0\n",
      "170     0\n",
      "2176    4\n",
      "957     2\n",
      "1835    4\n",
      "89      0\n",
      "1376    3\n",
      "1907    4\n",
      "1222    2\n",
      "1238    3\n",
      "1814    3\n",
      "1996    4\n",
      "2133    4\n",
      "2060    4\n",
      "1901    4\n",
      "362     0\n",
      "513     0\n",
      "1077    2\n",
      "1817    3\n",
      "2075    4\n",
      "641     1\n",
      "617     1\n",
      "499     0\n",
      "1263    2\n",
      "1926    4\n",
      "1218    2\n",
      "198     0\n",
      "757     1\n",
      "1054    2\n",
      "788     1\n",
      "107     0\n",
      "241     0\n",
      "1803    3\n",
      "1562    3\n",
      "1310    2\n",
      "673     1\n",
      "1976    4\n",
      "661     1\n",
      "1283    2\n",
      "1782    3\n",
      "1643    3\n",
      "2092    4\n",
      "876     1\n",
      "2038    4\n",
      "330     0\n",
      "1087    2\n",
      "1573    3\n",
      "2140    4\n",
      "301     0\n",
      "2020    4\n",
      "1136    2\n",
      "2197    4\n",
      "496     0\n",
      "220     0\n",
      "874     1\n",
      "1102    2\n",
      "1544    3\n",
      "689     1\n",
      "1986    4\n",
      "1571    3\n",
      "296     0\n",
      "1358    3\n",
      "653     1\n",
      "1248    2\n",
      "1822    3\n",
      "1347    3\n",
      "2089    4\n",
      "1513    3\n",
      "969     2\n",
      "1975    4\n",
      "399     0\n",
      "1531    3\n",
      "72      0\n",
      "849     1\n",
      "796     1\n",
      "149     0\n",
      "1335    2\n",
      "1036    2\n",
      "1994    4\n",
      "576     1\n",
      "1899    4\n",
      "823     1\n",
      "1484    3\n",
      "2120    4\n",
      "319     0\n",
      "1528    3\n",
      "799     1\n",
      "728     1\n",
      "1259    2\n",
      "1143    2\n",
      "179     0\n",
      "1520    3\n",
      "1350    3\n",
      "800     1\n",
      "2123    4\n",
      "121     0\n",
      "545     1\n",
      "2213    4\n",
      "87      0\n",
      "292     0\n",
      "1991    4\n",
      "745     1\n",
      "1698    3\n",
      "1928    4\n",
      "571     1\n",
      "126     0\n",
      "1113    2\n",
      "386     0\n",
      "497     0\n",
      "1122    2\n",
      "310     0\n",
      "58      0\n",
      "166     0\n",
      "1420    3\n",
      "1192    2\n",
      "968     2\n",
      "459     0\n",
      "2086    4\n",
      "1821    3\n",
      "2195    4\n",
      "1802    3\n",
      "1417    3\n",
      "1408    3\n",
      "771     1\n",
      "1668    3\n",
      "351     0\n",
      "525     1\n",
      "104     0\n",
      "1039    2\n",
      "851     1\n",
      "109     0\n",
      "2001    4\n",
      "1346    3\n",
      "1739    3\n",
      "366     0\n",
      "1707    3\n",
      "580     1\n",
      "2074    4\n",
      "2191    4\n",
      "1993    4\n",
      "1622    3\n",
      "1719    3\n",
      "1518    3\n",
      "195     0\n",
      "464     0\n",
      "1606    3\n",
      "27      0\n",
      "387     0\n",
      "1128    2\n",
      "971     2\n",
      "1393    3\n",
      "271     0\n",
      "712     1\n",
      "598     1\n",
      "935     2\n",
      "478     0\n",
      "944     2\n",
      "611     1\n",
      "168     0\n",
      "1086    2\n",
      "837     1\n",
      "677     1\n",
      "422     0\n",
      "1626    3\n",
      "934     2\n",
      "1663    3\n",
      "1594    3\n",
      "2111    4\n",
      "1786    3\n",
      "544     1\n",
      "662     1\n",
      "920     2\n",
      "1051    2\n",
      "596     1\n",
      "1645    3\n",
      "523     0\n",
      "1497    3\n",
      "843     1\n",
      "1613    3\n",
      "2190    4\n",
      "1765    3\n",
      "2032    4\n",
      "2186    4\n",
      "1494    3\n",
      "1116    2\n",
      "1538    3\n",
      "562     1\n",
      "13      0\n",
      "1876    4\n",
      "1211    2\n",
      "1325    2\n",
      "2100    4\n",
      "1299    2\n",
      "2028    4\n",
      "76      0\n",
      "1997    4\n",
      "964     2\n",
      "88      0\n",
      "755     1\n",
      "3       0\n",
      "1429    3\n",
      "2019    4\n",
      "1959    4\n",
      "2112    4\n",
      "2099    4\n",
      "692     1\n",
      "1789    3\n",
      "403     0\n",
      "450     0\n",
      "1641    3\n",
      "1933    4\n",
      "372     0\n",
      "407     0\n",
      "289     0\n",
      "1920    4\n",
      "1297    2\n",
      "1172    3\n",
      "1596    3\n",
      "585     1\n",
      "1014    2\n",
      "1588    3\n",
      "506     0\n",
      "1047    2\n",
      "1044    2\n",
      "602     1\n",
      "2215    4\n",
      "1167    2\n",
      "1053    2\n",
      "718     1\n",
      "1059    2\n",
      "672     1\n",
      "1578    3\n",
      "222     0\n",
      "640     1\n",
      "567     1\n",
      "84      0\n",
      "1660    3\n",
      "1147    2\n",
      "45      0\n",
      "607     1\n",
      "1416    3\n",
      "473     0\n",
      "40      0\n",
      "1542    3\n",
      "247     0\n",
      "2188    4\n",
      "655     1\n",
      "740     1\n",
      "334     0\n",
      "1873    4\n",
      "1776    3\n",
      "397     0\n",
      "154     0\n",
      "208     0\n",
      "1631    3\n",
      "315     3\n",
      "248     0\n",
      "592     1\n",
      "1145    2\n",
      "2098    4\n",
      "1865    4\n",
      "1811    3\n",
      "2008    4\n",
      "1295    2\n",
      "1867    4\n",
      "1080    2\n",
      "738     1\n",
      "977     2\n",
      "174     0\n",
      "998     2\n",
      "2124    4\n",
      "1442    3\n",
      "956     2\n",
      "2219    4\n",
      "2039    4\n",
      "1609    3\n",
      "992     2\n",
      "1061    2\n",
      "863     1\n",
      "2070    4\n",
      "1445    3\n",
      "1692    3\n",
      "522     1\n",
      "114     0\n",
      "1363    3\n",
      "1331    2\n",
      "769     1\n",
      "1853    4\n",
      "1885    4\n",
      "1804    3\n",
      "155     0\n",
      "1770    3\n",
      "1349    3\n",
      "577     1\n",
      "924     2\n",
      "294     3\n",
      "940     2\n",
      "1737    3\n",
      "981     2\n",
      "1257    2\n",
      "1120    2\n",
      "2148    4\n",
      "1337    2\n",
      "2085    4\n",
      "568     1\n",
      "1338    2\n",
      "1871    4\n",
      "308     0\n",
      "681     1\n",
      "682     1\n",
      "265     0\n",
      "212     0\n",
      "267     0\n",
      "156     1\n",
      "624     1\n",
      "1566    3\n",
      "119     0\n",
      "1963    4\n",
      "203     0\n",
      "480     0\n",
      "947     2\n",
      "970     2\n",
      "593     1\n",
      "1083    2\n",
      "1217    2\n",
      "1294    2\n",
      "1946    4\n",
      "2169    4\n",
      "1362    3\n",
      "148     0\n",
      "140     0\n",
      "1410    3\n",
      "29      0\n",
      "1856    4\n",
      "1419    3\n",
      "1945    4\n",
      "695     1\n",
      "1129    2\n",
      "1658    3\n",
      "664     1\n",
      "752     1\n",
      "1228    2\n",
      "1468    3\n",
      "557     1\n",
      "595     1\n",
      "1618    3\n",
      "881     3\n",
      "157     0\n",
      "1369    3\n",
      "597     1\n",
      "1078    2\n",
      "827     1\n",
      "1024    2\n",
      "137     0\n",
      "1960    4\n",
      "1290    2\n",
      "476     0\n",
      "958     2\n",
      "2137    4\n",
      "347     0\n",
      "1829    4\n",
      "1115    3\n",
      "75      0\n",
      "699     1\n",
      "563     1\n",
      "930     2\n",
      "2071    4\n",
      "1514    3\n",
      "2061    4\n",
      "418     0\n",
      "730     1\n",
      "630     1\n",
      "2183    4\n",
      "529     1\n",
      "1201    2\n",
      "903     1\n",
      "1430    3\n",
      "521     0\n",
      "1270    2\n",
      "894     1\n",
      "1450    3\n",
      "2110    4\n",
      "945     2\n",
      "1545    3\n",
      "1385    3\n",
      "277     0\n",
      "1395    3\n",
      "278     0\n",
      "634     1\n",
      "2181    4\n",
      "1913    4\n",
      "1139    2\n",
      "1733    3\n",
      "1838    4\n",
      "618     1\n",
      "414     0\n",
      "1504    3\n",
      "1458    3\n",
      "69      0\n",
      "1874    4\n",
      "67      0\n",
      "70      0\n",
      "2041    4\n",
      "1241    2\n",
      "564     1\n",
      "1232    2\n",
      "1936    4\n",
      "1746    3\n",
      "514     0\n",
      "1258    2\n",
      "1305    2\n",
      "2193    4\n",
      "8       0\n",
      "1927    4\n",
      "2171    4\n",
      "49      0\n",
      "1133    2\n",
      "2104    4\n",
      "1710    3\n",
      "1341    3\n",
      "1530    3\n",
      "722     1\n",
      "925     2\n",
      "1982    4\n",
      "457     0\n",
      "1123    2\n",
      "358     0\n",
      "1956    4\n",
      "1071    2\n",
      "1616    3\n",
      "1160    2\n",
      "1327    2\n",
      "1422    3\n",
      "184     0\n",
      "2031    4\n",
      "1526    3\n",
      "118     0\n",
      "1826    4\n",
      "1567    3\n",
      "1661    3\n",
      "666     1\n",
      "1153    2\n",
      "502     0\n",
      "866     1\n",
      "658     1\n",
      "841     1\n",
      "1431    3\n",
      "1186    2\n",
      "207     1\n",
      "1757    3\n",
      "2129    4\n",
      "1995    4\n",
      "1536    3\n",
      "1058    2\n",
      "839     1\n",
      "2059    4\n",
      "93      0\n",
      "733     1\n",
      "20      0\n",
      "1938    4\n",
      "1098    2\n",
      "448     0\n",
      "1       0\n",
      "1563    3\n",
      "172     0\n",
      "1860    4\n",
      "556     1\n",
      "797     1\n",
      "1713    3\n",
      "191     0\n",
      "240     0\n",
      "599     1\n",
      "1379    3\n",
      "359     0\n",
      "2043    4\n",
      "2078    4\n",
      "632     1\n",
      "555     1\n",
      "383     0\n",
      "1236    2\n",
      "526     1\n",
      "106     0\n",
      "2151    4\n",
      "451     0\n",
      "1948    4\n",
      "491     0\n",
      "587     1\n",
      "504     0\n",
      "2022    4\n",
      "2081    4\n",
      "1353    3\n",
      "1925    4\n",
      "2009    4\n",
      "178     0\n",
      "1234    2\n",
      "360     0\n",
      "468     0\n",
      "1026    2\n",
      "367     0\n",
      "1734    3\n",
      "2       0\n",
      "1082    2\n",
      "283     0\n",
      "652     1\n",
      "1989    4\n",
      "1066    2\n",
      "1370    3\n",
      "12      0\n",
      "41      0\n",
      "406     0\n",
      "1930    4\n",
      "1582    3\n",
      "295     0\n",
      "960     2\n",
      "1555    3\n",
      "1730    3\n",
      "1965    4\n",
      "262     0\n",
      "1085    2\n",
      "2210    4\n",
      "535     1\n",
      "1169    2\n",
      "1001    2\n",
      "541     1\n",
      "768     1\n",
      "2021    4\n",
      "1966    4\n",
      "1074    2\n",
      "1119    2\n",
      "1140    2\n",
      "258     0\n",
      "1922    4\n",
      "554     1\n",
      "2073    4\n",
      "167     0\n",
      "1816    3\n",
      "325     0\n",
      "1601    3\n",
      "1239    2\n",
      "912     2\n",
      "1747    3\n",
      "1832    4\n",
      "86      0\n",
      "1866    4\n",
      "955     2\n",
      "910     2\n",
      "932     2\n",
      "505     0\n",
      "1699    3\n",
      "1987    4\n",
      "425     0\n",
      "1939    4\n",
      "1894    4\n",
      "906     1\n",
      "1314    2\n",
      "744     1\n",
      "762     1\n",
      "615     1\n",
      "424     0\n",
      "763     1\n",
      "327     0\n",
      "2017    4\n",
      "1696    3\n",
      "1973    4\n",
      "1434    3\n",
      "472     0\n",
      "1842    4\n",
      "1207    2\n",
      "1163    2\n",
      "667     1\n",
      "1293    2\n",
      "1394    3\n",
      "1868    4\n",
      "890     1\n",
      "1289    2\n",
      "857     1\n",
      "1214    2\n",
      "305     0\n",
      "798     1\n",
      "1612    3\n",
      "548     1\n",
      "2220    4\n",
      "684     1\n",
      "500     0\n",
      "579     1\n",
      "1485    3\n",
      "2003    4\n",
      "1653    3\n",
      "737     1\n",
      "100     0\n",
      "2088    4\n",
      "883     1\n",
      "2024    4\n",
      "2002    4\n",
      "2057    4\n",
      "1882    4\n",
      "393     0\n",
      "1049    2\n",
      "1164    2\n",
      "1603    3\n",
      "1849    4\n",
      "219     0\n",
      "1836    4\n",
      "1022    2\n",
      "1743    3\n",
      "877     1\n",
      "249     0\n",
      "736     1\n",
      "1515    3\n",
      "441     0\n",
      "1210    2\n",
      "1787    3\n",
      "530     1\n",
      "1801    3\n",
      "707     1\n",
      "2131    4\n",
      "706     1\n",
      "5       0\n",
      "1005    2\n",
      "373     0\n",
      "1629    3\n",
      "21      0\n",
      "438     0\n",
      "1177    2\n",
      "647     1\n",
      "1686    3\n",
      "885     1\n",
      "687     1\n",
      "318     0\n",
      "698     1\n",
      "613     1\n",
      "854     1\n",
      "1685    3\n",
      "1155    2\n",
      "1855    4\n",
      "659     1\n",
      "1858    4\n",
      "2093    4\n",
      "73      0\n",
      "1179    2\n",
      "1893    4\n",
      "1216    2\n",
      "1020    2\n",
      "1806    3\n",
      "1516    3\n",
      "1465    3\n",
      "966     2\n",
      "926     2\n",
      "1794    3\n",
      "527     1\n",
      "186     0\n",
      "831     1\n",
      "1898    4\n",
      "921     2\n",
      "1386    3\n",
      "1171    2\n",
      "427     0\n",
      "2205    4\n",
      "1199    2\n",
      "551     1\n",
      "818     1\n",
      "1221    2\n",
      "1256    2\n",
      "24      0\n",
      "657     1\n",
      "1375    3\n",
      "2113    4\n",
      "1203    2\n",
      "1844    4\n",
      "1908    4\n",
      "676     1\n",
      "2096    4\n",
      "1041    2\n",
      "371     0\n",
      "199     0\n",
      "1678    3\n",
      "726     1\n",
      "1796    3\n",
      "1568    3\n",
      "1249    2\n",
      "453     0\n",
      "1266    2\n",
      "627     1\n",
      "54      0\n",
      "333     0\n",
      "1595    3\n",
      "1274    2\n",
      "105     0\n",
      "343     0\n",
      "648     1\n",
      "639     1\n",
      "1954    4\n",
      "1738    3\n",
      "216     0\n",
      "356     0\n",
      "915     2\n",
      "1904    4\n",
      "1581    3\n",
      "1004    2\n",
      "1624    3\n",
      "560     1\n",
      "264     0\n",
      "1045    2\n",
      "1302    2\n",
      "1837    4\n",
      "1112    2\n",
      "2064    4\n",
      "445     0\n",
      "1512    3\n",
      "2198    4\n",
      "928     2\n",
      "644     1\n",
      "2136    4\n",
      "1923    4\n",
      "979     2\n",
      "1943    4\n",
      "994     2\n",
      "553     1\n",
      "1436    3\n",
      "784     1\n",
      "501     0\n",
      "1030    2\n",
      "443     0\n",
      "1276    2\n",
      "1296    2\n",
      "162     0\n",
      "455     0\n",
      "62      0\n",
      "1880    4\n",
      "1681    3\n",
      "218     3\n",
      "227     0\n",
      "2117    4\n",
      "471     0\n",
      "1593    3\n",
      "1021    2\n",
      "536     1\n",
      "224     0\n",
      "735     1\n",
      "1917    4\n",
      "1111    2\n",
      "1714    3\n",
      "1753    3\n",
      "1910    4\n",
      "2047    4\n",
      "1586    3\n",
      "999     2\n",
      "720     1\n",
      "1333    2\n",
      "1543    3\n",
      "1610    3\n",
      "864     1\n",
      "590     1\n",
      "901     1\n",
      "1383    3\n",
      "1650    3\n",
      "606     1\n",
      "1708    3\n",
      "1974    4\n",
      "503     0\n",
      "1655    3\n",
      "2077    4\n",
      "886     1\n",
      "1724    3\n",
      "1680    3\n",
      "908     3\n",
      "2130    4\n",
      "317     0\n",
      "2161    4\n",
      "1019    2\n",
      "1340    2\n",
      "299     0\n",
      "1506    3\n",
      "201     0\n",
      "679     1\n",
      "2052    4\n",
      "1418    3\n",
      "47      0\n",
      "1471    3\n",
      "1764    3\n",
      "229     0\n",
      "1262    2\n",
      "1502    3\n",
      "1759    3\n",
      "512     0\n",
      "395     0\n",
      "2214    4\n",
      "1453    3\n",
      "853     1\n",
      "1093    2\n",
      "533     1\n",
      "1742    3\n",
      "2165    4\n",
      "2158    4\n",
      "1687    3\n",
      "2062    4\n",
      "303     0\n",
      "2026    4\n",
      "808     1\n",
      "777     1\n",
      "538     1\n",
      "1339    2\n",
      "1736    3\n",
      "1491    3\n",
      "869     1\n",
      "1953    4\n",
      "703     1\n",
      "494     0\n",
      "419     0\n",
      "1503    3\n",
      "842     1\n",
      "268     0\n",
      "2200    4\n",
      "1328    2\n",
      "382     0\n",
      "2126    4\n",
      "1951    4\n",
      "342     0\n",
      "1096    2\n",
      "2196    4\n",
      "670     1\n",
      "146     0\n",
      "188     0\n",
      "1912    4\n",
      "1805    3\n",
      "609     3\n",
      "122     0\n",
      "1607    3\n",
      "1565    3\n",
      "420     0\n",
      "1628    3\n",
      "132     0\n",
      "973     2\n",
      "1792    3\n",
      "2155    4\n",
      "1881    4\n",
      "324     0\n",
      "1159    2\n",
      "727     1\n",
      "972     2\n",
      "1246    2\n",
      "245     0\n",
      "1285    3\n",
      "435     0\n",
      "483     0\n",
      "2063    4\n",
      "1448    3\n",
      "253     0\n",
      "1919    4\n",
      "1955    4\n",
      "316     0\n",
      "10      0\n",
      "1423    3\n",
      "1522    3\n",
      "1788    3\n",
      "1682    3\n",
      "463     0\n",
      "1242    2\n",
      "693     1\n",
      "650     1\n",
      "489     0\n",
      "1322    2\n",
      "1243    2\n",
      "111     0\n",
      "1583    3\n",
      "1671    3\n",
      "1540    3\n",
      "674     1\n",
      "1181    2\n",
      "1372    3\n",
      "43      0\n",
      "77      0\n",
      "461     0\n",
      "1467    3\n",
      "272     3\n",
      "1064    2\n",
      "1070    2\n",
      "1268    2\n",
      "1251    3\n",
      "780     1\n",
      "2152    4\n",
      "1846    4\n",
      "300     0\n",
      "1287    2\n",
      "889     1\n",
      "1754    3\n",
      "939     2\n",
      "913     2\n",
      "63      0\n",
      "1165    2\n",
      "55      0\n",
      "1541    3\n",
      "26      0\n",
      "569     1\n",
      "1361    3\n",
      "1950    4\n",
      "276     0\n",
      "1130    2\n",
      "678     1\n",
      "537     1\n",
      "1094    2\n",
      "2182    4\n",
      "481     0\n",
      "1679    3\n",
      "209     0\n",
      "1073    2\n",
      "346     0\n",
      "812     1\n",
      "1368    3\n",
      "39      0\n",
      "980     2\n",
      "1892    4\n",
      "1300    2\n",
      "860     1\n",
      "896     1\n",
      "1398    3\n",
      "1090    2\n",
      "454     0\n",
      "1230    2\n",
      "663     1\n",
      "287     0\n",
      "605     1\n",
      "1783    3\n",
      "433     0\n",
      "1673    3\n",
      "1647    3\n",
      "312     0\n",
      "158     0\n",
      "409     0\n",
      "697     1\n",
      "2202    4\n",
      "1461    3\n",
      "235     0\n",
      "1840    4\n",
      "400     0\n",
      "996     2\n",
      "510     0\n",
      "995     2\n",
      "462     0\n",
      "710     1\n",
      "2146    4\n",
      "434     0\n",
      "2135    4\n",
      "231     0\n",
      "221     0\n",
      "22      0\n",
      "410     0\n",
      "385     0\n",
      "36      0\n",
      "2116    4\n",
      "2000    4\n",
      "1065    2\n",
      "1615    3\n",
      "452     0\n",
      "1559    3\n",
      "1553    3\n",
      "214     0\n",
      "2040    4\n",
      "2184    4\n",
      "801     1\n",
      "1198    2\n",
      "306     0\n",
      "152     0\n",
      "396     0\n",
      "11      0\n",
      "1012    2\n",
      "440     0\n",
      "2154    4\n",
      "550     1\n",
      "2044    4\n",
      "1250    2\n",
      "1202    2\n",
      "1852    4\n",
      "1016    2\n",
      "2168    4\n",
      "449     0\n",
      "1117    2\n",
      "1144    2\n",
      "835     1\n",
      "98      0\n",
      "1157    2\n",
      "1191    2\n",
      "1810    3\n",
      "2209    4\n",
      "1288    2\n",
      "1828    4\n",
      "621     1\n",
      "446     0\n",
      "902     1\n",
      "1219    2\n",
      "1740    3\n",
      "391     0\n",
      "1556    3\n",
      "1132    2\n",
      "753     1\n",
      "321     0\n",
      "1367    3\n",
      "975     2\n",
      "636     3\n",
      "2199    4\n",
      "751     1\n",
      "1298    2\n",
      "829     1\n",
      "1883    4\n",
      "1533    3\n",
      "1052    2\n",
      "1599    3\n",
      "1316    2\n",
      "337     0\n",
      "1443    3\n",
      "59      0\n",
      "1435    3\n",
      "1918    4\n",
      "767     1\n",
      "2094    4\n",
      "1011    2\n",
      "1455    3\n",
      "1101    2\n",
      "1176    2\n",
      "1768    3\n",
      "444     0\n",
      "700     1\n",
      "1902    4\n",
      "1457    3\n",
      "181     3\n",
      "123     0\n",
      "1208    2\n",
      "2083    4\n",
      "1100    2\n",
      "1861    4\n",
      "570     1\n",
      "1589    3\n",
      "1854    4\n",
      "2170    4\n",
      "887     1\n",
      "431     0\n",
      "2153    4\n",
      "270     0\n",
      "2166    4\n",
      "635     1\n",
      "1226    2\n",
      "982     2\n",
      "1126    2\n",
      "1635    3\n",
      "1018    2\n",
      "2216    4\n",
      "1758    3\n",
      "1957    4\n",
      "470     0\n",
      "408     0\n",
      "1495    3\n",
      "1990    4\n",
      "1909    4\n",
      "288     0\n",
      "291     0\n",
      "1384    3\n",
      "855     1\n",
      "1630    3\n",
      "990     2\n",
      "256     0\n",
      "285     0\n",
      "714     1\n",
      "14      0\n",
      "133     0\n",
      "713     1\n",
      "1889    4\n",
      "42      0\n",
      "1043    2\n",
      "511     0\n",
      "721     1\n",
      "1691    3\n",
      "1970    4\n",
      "1286    2\n",
      "1421    3\n",
      "1089    2\n",
      "1106    2\n",
      "1377    3\n",
      "2010    4\n",
      "764     1\n",
      "2103    4\n",
      "436     0\n",
      "821     1\n",
      "1791    3\n",
      "1397    3\n",
      "175     0\n",
      "251     0\n",
      "1149    2\n",
      "1146    2\n",
      "1235    2\n",
      "1121    2\n",
      "668     1\n",
      "466     0\n",
      "1247    2\n",
      "559     1\n",
      "180     0\n",
      "1312    2\n",
      "750     1\n",
      "2101    4\n",
      "1715    3\n",
      "2157    4\n",
      "2156    4\n",
      "2087    4\n",
      "1284    2\n",
      "2091    4\n",
      "269     0\n",
      "1633    3\n",
      "1197    2\n",
      "1972    4\n",
      "937     2\n",
      "1638    3\n",
      "1623    3\n",
      "131     0\n",
      "845     1\n",
      "2178    4\n",
      "429     0\n",
      "1863    4\n",
      "19      0\n",
      "2194    4\n",
      "1627    3\n",
      "1473    3\n",
      "1947    4\n",
      "1751    3\n",
      "1015    2\n",
      "18      0\n",
      "442     0\n",
      "1843    4\n",
      "1313    2\n",
      "474     0\n",
      "33      0\n",
      "1224    2\n",
      "1744    3\n",
      "1646    3\n",
      "1729    3\n",
      "1427    3\n",
      "1887    4\n",
      "819     1\n",
      "1391    3\n",
      "1056    2\n",
      "1940    4\n",
      "783     1\n",
      "815     1\n",
      "164     3\n",
      "225     0\n",
      "1763    3\n",
      "82      0\n",
      "357     0\n",
      "573     1\n",
      "1508    3\n",
      "1726    3\n",
      "1756    3\n",
      "916     2\n",
      "742     1\n",
      "1447    3\n",
      "1978    4\n",
      "1934    4\n",
      "897     3\n",
      "1958    4\n",
      "2187    4\n",
      "1695    3\n",
      "250     0\n",
      "147     0\n",
      "298     0\n",
      "997     2\n",
      "1401    3\n",
      "811     1\n",
      "1029    2\n",
      "1640    3\n",
      "1233    2\n",
      "716     1\n",
      "575     1\n",
      "1932    4\n",
      "717     1\n",
      "187     1\n",
      "1194    2\n",
      "583     1\n",
      "1307    2\n",
      "293     3\n",
      "99      0\n",
      "477     0\n",
      "2138    4\n",
      "1684    3\n",
      "1326    2\n",
      "1142    2\n",
      "929     2\n",
      "1482    3\n",
      "951     2\n",
      "51      0\n",
      "814     1\n",
      "2222    4\n",
      "1462    3\n",
      "376     0\n",
      "1634    3\n",
      "1114    2\n",
      "1690    3\n",
      "1683    3\n",
      "2115    4\n",
      "1833    4\n",
      "465     0\n",
      "242     0\n",
      "790     1\n",
      "223     0\n",
      "1600    3\n",
      "991     2\n",
      "2053    4\n",
      "1261    2\n",
      "159     0\n",
      "2149    4\n",
      "1387    3\n",
      "1356    3\n",
      "1412    3\n",
      "171     0\n",
      "1025    2\n",
      "1632    3\n",
      "2150    4\n",
      "161     0\n",
      "1550    3\n",
      "1184    2\n",
      "731     1\n",
      "1206    2\n",
      "1644    3\n",
      "485     0\n",
      "786     1\n",
      "2145    4\n",
      "948     2\n",
      "952     2\n",
      "1772    3\n",
      "1137    2\n",
      "961     2\n",
      "1317    2\n",
      "1003    2\n",
      "820     1\n",
      "1611    3\n",
      "1033    2\n",
      "228     0\n",
      "404     0\n",
      "482     0\n",
      "415     0\n",
      "1721    3\n",
      "1403    3\n",
      "486     0\n",
      "1452    3\n",
      "490     0\n",
      "2185    4\n",
      "1585    3\n",
      "1941    4\n",
      "552     1\n",
      "1771    3\n",
      "2121    4\n",
      "1501    3\n",
      "2164    4\n",
      "1480    3\n",
      "177     0\n",
      "412     0\n",
      "1900    4\n",
      "353     0\n",
      "1304    2\n",
      "1255    2\n",
      "772     1\n",
      "141     0\n",
      "1264    2\n",
      "1830    4\n",
      "57      0\n",
      "81      0\n",
      "1674    3\n",
      "1725    3\n",
      "734     1\n",
      "719     1\n",
      "574     1\n",
      "236     0\n",
      "770     1\n",
      "1057    2\n",
      "211     0\n",
      "804     1\n",
      "779     1\n",
      "402     0\n",
      "1774    3\n",
      "1472    3\n",
      "6       0\n",
      "96      0\n",
      "417     0\n",
      "729     1\n",
      "620     1\n",
      "683     1\n",
      "1689    3\n",
      "1048    2\n",
      "279     0\n",
      "1850    4\n",
      "1741    3\n",
      "46      0\n",
      "498     0\n",
      "322     3\n",
      "588     1\n",
      "1354    3\n",
      "25      0\n",
      "378     0\n",
      "135     0\n",
      "600     1\n",
      "1245    3\n",
      "2004    4\n",
      "475     0\n",
      "691     1\n",
      "1183    2\n",
      "1475    3\n",
      "1127    2\n",
      "2127    4\n",
      "313     0\n",
      "380     0\n",
      "1525    3\n",
      "1438    3\n",
      "2036    4\n",
      "1572    3\n",
      "1308    2\n",
      "1193    2\n",
      "1574    3\n",
      "1411    3\n",
      "384     0\n",
      "7       0\n",
      "1466    3\n",
      "1154    2\n",
      "1359    3\n",
      "1097    2\n",
      "589     1\n",
      "748     1\n",
      "1223    2\n",
      "987     2\n",
      "1498    3\n",
      "1834    4\n",
      "1424    3\n",
      "492     0\n",
      "846     1\n",
      "933     2\n",
      "1320    2\n",
      "311     0\n",
      "200     0\n",
      "817     1\n",
      "1253    2\n",
      "1141    2\n",
      "656     1\n",
      "540     1\n",
      "1937    4\n",
      "1984    4\n",
      "66      0\n",
      "1437    3\n",
      "1621    3\n",
      "833     1\n",
      "1170    2\n",
      "1755    3\n",
      "388     0\n",
      "517     0\n",
      "183     0\n",
      "1539    3\n",
      "423     0\n",
      "1706    3\n",
      "671     1\n",
      "1446    3\n",
      "1614    3\n",
      "1134    2\n",
      "35      0\n",
      "232     0\n",
      "1035    2\n",
      "30      0\n",
      "1229    2\n",
      "724     1\n",
      "2212    4\n",
      "1648    3\n",
      "680     1\n",
      "1924    4\n",
      "1597    3\n",
      "64      0\n",
      "2192    4\n",
      "1483    3\n",
      "875     1\n",
      "1008    2\n",
      "2102    4\n",
      "364     0\n",
      "927     2\n",
      "1381    3\n",
      "2029    4\n",
      "907     1\n",
      "79      0\n",
      "1213    2\n",
      "610     1\n",
      "1760    3\n",
      "1131    2\n",
      "1558    3\n",
      "1728    3\n",
      "1905    4\n",
      "898     1\n",
      "856     1\n",
      "331     0\n",
      "134     0\n",
      "633     1\n",
      "381     0\n",
      "192     0\n",
      "101     0\n",
      "1402    3\n",
      "1108    2\n",
      "266     0\n",
      "1319    2\n",
      "1716    3\n",
      "1931    4\n",
      "1464    3\n",
      "2046    4\n",
      "456     0\n",
      "758     1\n",
      "1537    3\n",
      "1929    4\n",
      "1104    2\n",
      "623     1\n",
      "1330    2\n",
      "638     1\n",
      "1188    2\n",
      "2037    4\n",
      "1723    3\n",
      "715     1\n",
      "205     0\n",
      "2048    4\n",
      "165     0\n",
      "1063    2\n",
      "1023    2\n",
      "1800    3\n",
      "1488    3\n",
      "1591    3\n",
      "1038    2\n",
      "2050    4\n",
      "696     1\n",
      "163     0\n",
      "892     1\n",
      "508     0\n",
      "2180    4\n",
      "182     0\n",
      "1819    3\n",
      "1150    2\n",
      "2177    4\n",
      "1785    3\n",
      "1454    3\n",
      "1569    3\n",
      "1138    2\n",
      "1507    3\n",
      "1315    2\n",
      "1006    2\n",
      "2058    4\n",
      "1277    2\n",
      "2147    4\n",
      "967     2\n",
      "813     1\n",
      "628     1\n",
      "2141    4\n",
      "594     1\n",
      "2143    4\n",
      "649     1\n",
      "263     0\n",
      "1031    2\n",
      "124     0\n",
      "1703    3\n",
      "458     0\n",
      "983     2\n",
      "1161    2\n",
      "1667    3\n",
      "895     1\n",
      "539     1\n",
      "259     0\n",
      "348     0\n",
      "1000    2\n",
      "1701    3\n",
      "150     0\n",
      "1156    2\n",
      "1318    2\n",
      "702     1\n",
      "1432    3\n",
      "2109    4\n",
      "918     2\n",
      "90      0\n",
      "493     0\n",
      "1013    2\n",
      "1007    2\n",
      "1717    3\n",
      "756     1\n",
      "1968    4\n",
      "151     1\n",
      "785     1\n",
      "2030    4\n",
      "1579    3\n",
      "169     1\n",
      "665     1\n",
      "1105    2\n",
      "694     1\n",
      "1042    2\n",
      "1152    2\n",
      "1560    3\n",
      "439     0\n",
      "484     0\n",
      "1440    3\n",
      "1451    3\n",
      "1178    2\n",
      "1040    2\n",
      "1637    3\n",
      "1839    4\n",
      "426     0\n",
      "1390    3\n",
      "94      0\n",
      "1935    4\n",
      "350     0\n",
      "1527    3\n",
      "803     1\n",
      "942     2\n",
      "1265    2\n",
      "1081    2\n",
      "775     1\n",
      "946     2\n",
      "1205    2\n",
      "873     1\n",
      "1554    3\n",
      "1869    4\n",
      "1651    3\n",
      "48      0\n",
      "586     1\n",
      "1677    3\n",
      "112     0\n",
      "1773    3\n",
      "1564    3\n",
      "1364    3\n",
      "1099    2\n",
      "1357    3\n",
      "341     0\n",
      "2160    4\n",
      "1345    3\n",
      "1700    3\n",
      "905     1\n",
      "1841    4\n",
      "518     0\n",
      "1382    3\n",
      "732     1\n",
      "239     0\n",
      "1463    3\n",
      "1809    3\n",
      "1797    3\n",
      "865     1\n",
      "688     1\n",
      "1694    3\n",
      "309     0\n",
      "1878    4\n",
      "284     0\n",
      "432     0\n",
      "1204    2\n",
      "1225    3\n",
      "1604    3\n",
      "2023    4\n",
      "1547    3\n",
      "1884    4\n",
      "2005    4\n",
      "1827    4\n",
      "1413    3\n",
      "1587    3\n",
      "1769    3\n",
      "1820    3\n",
      "572     1\n",
      "776     1\n",
      "2114    4\n",
      "616     1\n",
      "1212    3\n",
      "1032    2\n",
      "129     0\n",
      "9       0\n",
      "1577    3\n",
      "363     0\n",
      "377     0\n",
      "394     0\n",
      "622     1\n",
      "1396    3\n",
      "52      0\n",
      "2107    4\n",
      "257     0\n",
      "1704    3\n",
      "467     0\n",
      "1720    3\n",
      "1608    3\n",
      "2006    4\n",
      "1076    2\n",
      "2056    4\n",
      "723     1\n",
      "654     1\n",
      "204     0\n",
      "1659    3\n",
      "243     0\n",
      "806     1\n",
      "1166    2\n",
      "1675    3\n",
      "297     0\n",
      "1118    2\n",
      "2072    4\n",
      "144     0\n",
      "1444    3\n",
      "509     0\n",
      "302     0\n",
      "1636    3\n",
      "985     2\n",
      "1195    3\n",
      "631     1\n",
      "1441    3\n",
      "1886    4\n",
      "637     1\n",
      "2025    4\n",
      "2174    4\n",
      "345     0\n",
      "1983    4\n",
      "401     0\n",
      "2122    4\n",
      "1329    2\n",
      "173     0\n",
      "252     0\n",
      "61      0\n",
      "1348    3\n",
      "861     1\n",
      "261     0\n",
      "936     3\n",
      "660     1\n",
      "778     1\n",
      "619     1\n",
      "872     1\n",
      "782     1\n",
      "1664    3\n",
      "520     0\n",
      "15      0\n",
      "1592    3\n",
      "868     1\n",
      "1549    3\n",
      "1625    3\n",
      "848     1\n",
      "962     2\n",
      "1845    4\n",
      "1220    2\n",
      "1575    3\n",
      "1870    4\n",
      "1778    3\n",
      "601     1\n",
      "428     0\n",
      "430     0\n",
      "1576    3\n",
      "963     2\n",
      "1271    2\n",
      "1469    3\n",
      "1095    2\n",
      "153     0\n",
      "824     1\n",
      "1135    3\n",
      "1426    3\n",
      "280     0\n",
      "1551    3\n",
      "531     1\n",
      "1509    3\n",
      "1534    3\n",
      "110     0\n",
      "1652    3\n",
      "705     1\n",
      "993     2\n",
      "1711    3\n",
      "1405    3\n",
      "2159    4\n",
      "1237    3\n",
      "743     1\n",
      "765     1\n",
      "2034    4\n",
      "1896    4\n",
      "1519    3\n",
      "1067    2\n",
      "1037    2\n",
      "1702    3\n",
      "794     1\n",
      "774     1\n",
      "690     1\n",
      "217     0\n",
      "1279    2\n",
      "1979    4\n",
      "97      0\n",
      "Name: category_target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking what they look like\n",
    "print(\"--- Vectorized Features for Training:\")\n",
    "print(features_train)\n",
    "print(\"--- Target Labels:\")\n",
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-foundation",
   "metadata": {},
   "source": [
    "**We now have our sparse matrices ready to being used for Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-german",
   "metadata": {},
   "source": [
    "## Classification: Using Random Forest Model (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "assured-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "parallel-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_rf = RandomForestClassifier(random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-thousand",
   "metadata": {},
   "source": [
    "**TODO: Do some hyperparameters fine-tuning steps here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sophisticated-routine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=777)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_rf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "instrumental-examination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 3, 0, 2, 3, 3, 1, 0, 2, 2, 4, 2, 3, 2, 1, 1, 3, 3, 3,\n",
       "       1, 0, 3, 1, 0, 2, 1, 0, 3, 0, 1, 4, 0, 0, 0, 0, 3, 1, 2, 3, 4, 0,\n",
       "       1, 2, 0, 1, 0, 1, 1, 2, 3, 1, 3, 3, 0, 4, 0, 4, 3, 3, 2, 3, 1, 3,\n",
       "       3, 1, 4, 1, 4, 0, 4, 4, 0, 3, 1, 2, 3, 0, 3, 2, 4, 4, 2, 3, 3, 0,\n",
       "       0, 2, 4, 2, 3, 0, 4, 4, 3, 3, 1, 0, 3, 3, 3, 2, 4, 0, 0, 2, 2, 3,\n",
       "       4, 3, 0, 1, 2, 4, 4, 0, 3, 3, 3, 0, 2, 0, 3, 1, 3, 3, 1, 0, 0, 4,\n",
       "       2, 4, 3, 0, 2, 0, 3, 2, 1, 2, 2, 1, 3, 0, 3, 3, 1, 3, 1, 0, 3, 2,\n",
       "       3, 2, 1, 3, 2, 2, 4, 3, 3, 0, 2, 0, 1, 1, 3, 4, 0, 2, 4, 3, 2, 0,\n",
       "       1, 0, 4, 4, 3, 1, 2, 1, 4, 0, 4, 0, 0, 2, 0, 0, 0, 1, 4, 0, 3, 4,\n",
       "       3, 0, 1, 1, 0, 0, 4, 0, 3, 2, 0, 1, 1, 4, 4, 0, 0, 3, 0, 3, 0, 2,\n",
       "       1, 0, 0, 1, 3, 2, 4, 1, 4, 4, 3, 0, 3, 3, 3, 3, 1, 2, 3, 0, 3, 2,\n",
       "       1, 1, 2, 0, 1, 4, 0, 3, 1, 2, 1, 1, 2, 2, 1, 3, 3, 3, 3, 2, 0, 2,\n",
       "       2, 3, 1, 3, 0, 3, 3, 3, 4, 1, 1, 0, 2, 0, 4, 0, 3, 0, 0, 4, 3, 4,\n",
       "       0, 0, 3, 2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 4, 0, 0, 4, 3, 3, 2, 1, 0,\n",
       "       0, 2, 1, 2, 2, 3, 4, 1, 4, 0, 1, 3, 4, 3, 3, 3, 3, 3, 0, 1, 2, 3,\n",
       "       4, 4, 1, 0, 4, 1, 2, 1, 0, 2, 4, 4, 2, 1, 0, 4, 4, 3, 4, 1, 1, 2,\n",
       "       4, 2, 1, 3, 3, 0, 2, 2, 3, 3, 2, 2, 0, 2, 3, 2, 3, 2, 0, 0, 2, 3,\n",
       "       3, 0, 4, 2, 4, 0, 3, 3, 0, 4, 4, 3, 4, 2, 0, 1, 0, 1, 0, 0, 0, 4,\n",
       "       2, 3, 2, 0, 0, 2, 0, 4, 4, 1, 4, 3, 0, 2, 0, 4, 0, 0, 0, 3, 0, 3,\n",
       "       4, 0, 0, 0, 2, 0, 4, 2, 2, 3, 0, 0, 1, 3, 3, 4, 3, 2, 0, 2, 3, 1,\n",
       "       1, 1, 2, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "labels_pred = model_rf.predict(features_test)\n",
    "display(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-hamburg",
   "metadata": {},
   "source": [
    "### Random Forest Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "amateur-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9483146067415731\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "premium-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       102\n",
      "           1       0.99      0.94      0.96        77\n",
      "           2       0.96      0.95      0.96        84\n",
      "           3       0.91      0.99      0.95       102\n",
      "           4       1.00      0.84      0.91        80\n",
      "\n",
      "    accuracy                           0.95       445\n",
      "   macro avg       0.96      0.94      0.95       445\n",
      "weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "report = classification_report(labels_test, labels_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-costs",
   "metadata": {},
   "source": [
    "## Classification: Using Logistic Regression Model (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "engaged-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "express-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-bench",
   "metadata": {},
   "source": [
    "**TODO: Do some hyperparameters fine-tuning steps here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "injured-canadian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_lr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pretty-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 3, 0, 2, 3, 3, 1, 0, 2, 2, 4, 2, 3, 2, 1, 1, 3, 3, 3,\n",
       "       1, 0, 3, 1, 0, 2, 1, 0, 3, 0, 1, 4, 0, 0, 0, 0, 3, 1, 2, 3, 4, 0,\n",
       "       1, 2, 0, 1, 0, 1, 1, 2, 3, 1, 3, 3, 0, 4, 0, 4, 3, 3, 2, 3, 1, 3,\n",
       "       3, 1, 4, 1, 4, 0, 4, 4, 0, 3, 1, 2, 3, 0, 3, 2, 4, 4, 2, 3, 3, 0,\n",
       "       0, 2, 4, 2, 3, 0, 4, 4, 3, 3, 1, 0, 3, 3, 3, 2, 4, 0, 0, 2, 2, 3,\n",
       "       4, 3, 0, 1, 2, 4, 4, 0, 3, 3, 3, 0, 2, 0, 3, 1, 3, 3, 1, 2, 0, 4,\n",
       "       2, 4, 3, 0, 2, 0, 3, 2, 1, 2, 2, 2, 3, 0, 3, 3, 1, 4, 1, 0, 3, 2,\n",
       "       3, 2, 1, 3, 2, 2, 4, 1, 3, 0, 2, 0, 1, 1, 3, 4, 0, 2, 4, 3, 2, 0,\n",
       "       1, 0, 4, 4, 3, 1, 2, 1, 4, 0, 4, 0, 0, 2, 0, 0, 0, 1, 4, 0, 3, 4,\n",
       "       3, 0, 1, 1, 0, 1, 4, 0, 3, 2, 0, 1, 1, 4, 4, 0, 0, 3, 0, 3, 0, 2,\n",
       "       1, 0, 0, 1, 3, 2, 4, 1, 2, 4, 4, 0, 3, 3, 3, 3, 1, 2, 3, 0, 3, 2,\n",
       "       1, 1, 2, 0, 1, 4, 0, 3, 1, 2, 1, 1, 2, 2, 1, 3, 3, 3, 3, 2, 0, 2,\n",
       "       2, 2, 1, 3, 0, 4, 4, 3, 4, 1, 1, 0, 2, 0, 4, 0, 3, 0, 0, 4, 3, 4,\n",
       "       0, 0, 3, 2, 1, 0, 2, 1, 1, 2, 2, 2, 1, 4, 0, 0, 4, 3, 3, 2, 1, 0,\n",
       "       0, 2, 1, 2, 2, 3, 4, 1, 4, 0, 1, 3, 4, 3, 3, 3, 3, 3, 0, 1, 2, 3,\n",
       "       4, 4, 1, 0, 4, 1, 2, 1, 0, 2, 4, 4, 2, 1, 0, 4, 4, 3, 4, 1, 1, 2,\n",
       "       4, 2, 1, 3, 3, 4, 2, 2, 3, 3, 2, 2, 0, 2, 3, 1, 3, 2, 0, 0, 2, 3,\n",
       "       3, 0, 4, 2, 4, 0, 3, 3, 0, 4, 4, 3, 4, 2, 0, 2, 0, 1, 2, 2, 0, 4,\n",
       "       2, 3, 3, 0, 0, 2, 0, 4, 4, 1, 4, 3, 0, 2, 0, 4, 0, 0, 0, 3, 0, 3,\n",
       "       0, 0, 0, 0, 2, 4, 4, 2, 2, 3, 0, 0, 1, 3, 3, 4, 3, 2, 0, 2, 3, 1,\n",
       "       1, 1, 2, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "labels_pred = model_lr.predict(features_test)\n",
    "display(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-opinion",
   "metadata": {},
   "source": [
    "### Logistic Regression Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "another-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sweet-familiar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       102\n",
      "           1       1.00      0.97      0.99        77\n",
      "           2       0.95      1.00      0.98        84\n",
      "           3       0.96      1.00      0.98       102\n",
      "           4       1.00      0.89      0.94        80\n",
      "\n",
      "    accuracy                           0.97       445\n",
      "   macro avg       0.98      0.97      0.97       445\n",
      "weighted avg       0.97      0.97      0.97       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "report = classification_report(labels_test, labels_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-october",
   "metadata": {},
   "source": [
    "## Classification: Using K-Nearest Neighbors Model (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "natural-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "completed-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-ideal",
   "metadata": {},
   "source": [
    "**TODO: Do some hyperparameters fine-tuning steps here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ultimate-editor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_knn.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "attempted-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 3, 0, 2, 3, 3, 1, 0, 2, 2, 4, 2, 3, 2, 1, 1, 3, 3, 3,\n",
       "       1, 4, 3, 1, 0, 2, 1, 0, 3, 4, 1, 4, 0, 0, 0, 0, 3, 1, 2, 3, 4, 0,\n",
       "       1, 2, 0, 1, 0, 1, 1, 2, 3, 1, 3, 3, 0, 4, 0, 4, 3, 3, 2, 3, 1, 3,\n",
       "       3, 1, 4, 1, 4, 0, 4, 1, 0, 3, 1, 2, 3, 0, 3, 2, 4, 4, 2, 3, 3, 0,\n",
       "       0, 2, 4, 2, 3, 0, 4, 4, 3, 3, 1, 0, 3, 3, 3, 2, 4, 0, 0, 2, 2, 3,\n",
       "       4, 3, 0, 1, 2, 4, 4, 0, 3, 3, 3, 0, 2, 0, 3, 1, 3, 3, 1, 2, 0, 4,\n",
       "       2, 4, 3, 0, 2, 0, 3, 2, 1, 2, 2, 1, 3, 0, 3, 3, 1, 4, 1, 0, 3, 2,\n",
       "       3, 2, 1, 3, 2, 2, 4, 1, 3, 0, 2, 0, 1, 1, 3, 4, 0, 2, 4, 3, 2, 0,\n",
       "       1, 0, 4, 4, 3, 1, 2, 1, 4, 0, 4, 0, 0, 2, 0, 0, 0, 1, 4, 0, 3, 4,\n",
       "       3, 0, 1, 1, 0, 1, 4, 4, 3, 2, 0, 1, 1, 4, 4, 0, 0, 3, 0, 3, 0, 2,\n",
       "       1, 0, 0, 1, 3, 2, 4, 1, 2, 4, 4, 0, 3, 3, 3, 3, 1, 2, 3, 0, 3, 2,\n",
       "       1, 1, 2, 0, 1, 4, 0, 3, 1, 2, 1, 1, 2, 2, 1, 3, 3, 3, 3, 2, 0, 4,\n",
       "       2, 2, 1, 3, 2, 4, 4, 3, 4, 1, 1, 0, 2, 0, 4, 0, 3, 0, 0, 4, 3, 4,\n",
       "       0, 0, 3, 2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 4, 0, 0, 4, 3, 3, 2, 1, 0,\n",
       "       0, 2, 1, 2, 2, 3, 4, 1, 4, 0, 1, 4, 4, 3, 3, 3, 3, 3, 0, 1, 2, 3,\n",
       "       4, 4, 1, 0, 4, 1, 2, 1, 0, 2, 4, 4, 2, 1, 0, 4, 4, 3, 4, 1, 1, 2,\n",
       "       4, 2, 1, 3, 3, 4, 2, 2, 3, 3, 2, 2, 0, 2, 3, 1, 4, 2, 0, 0, 2, 3,\n",
       "       3, 0, 4, 2, 0, 0, 3, 3, 0, 4, 4, 3, 4, 2, 0, 4, 1, 1, 2, 2, 0, 4,\n",
       "       2, 3, 3, 0, 0, 2, 0, 4, 4, 1, 4, 3, 0, 2, 0, 4, 0, 0, 0, 3, 0, 3,\n",
       "       4, 0, 0, 0, 2, 3, 4, 2, 2, 3, 0, 0, 1, 3, 3, 4, 3, 2, 0, 2, 3, 1,\n",
       "       1, 1, 2, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "labels_pred = model_knn.predict(features_test)\n",
    "display(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-subscriber",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "second-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9662921348314607\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "preliminary-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       102\n",
      "           1       0.99      0.99      0.99        77\n",
      "           2       0.97      0.99      0.98        84\n",
      "           3       0.97      1.00      0.99       102\n",
      "           4       0.95      0.90      0.92        80\n",
      "\n",
      "    accuracy                           0.97       445\n",
      "   macro avg       0.97      0.97      0.97       445\n",
      "weighted avg       0.97      0.97      0.97       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "report = classification_report(labels_test, labels_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-proceeding",
   "metadata": {},
   "source": [
    "## Classification: Using Simple Decision Tree Model (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "hundred-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sorted-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-malaysia",
   "metadata": {},
   "source": [
    "**TODO: Do some hyperparameters fine-tuning steps here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "victorian-leeds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_dt.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "honest-regular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 0, 3, 3, 0, 2, 3, 3, 1, 3, 2, 2, 4, 2, 3, 0, 1, 4, 3, 3, 3,\n",
       "       4, 4, 3, 4, 0, 0, 1, 0, 3, 0, 1, 4, 0, 0, 0, 0, 3, 1, 2, 3, 4, 0,\n",
       "       1, 4, 4, 1, 0, 1, 4, 2, 3, 3, 3, 2, 1, 4, 0, 0, 4, 3, 0, 2, 1, 3,\n",
       "       3, 1, 4, 1, 4, 4, 4, 4, 0, 3, 0, 2, 3, 2, 3, 2, 4, 4, 2, 3, 3, 0,\n",
       "       0, 2, 4, 2, 2, 0, 4, 3, 3, 3, 1, 0, 3, 3, 3, 2, 4, 0, 0, 2, 2, 3,\n",
       "       4, 3, 0, 0, 2, 4, 4, 0, 3, 3, 3, 2, 2, 0, 3, 1, 3, 3, 0, 3, 0, 4,\n",
       "       2, 0, 3, 0, 1, 0, 3, 0, 1, 2, 2, 1, 3, 0, 3, 3, 1, 4, 1, 0, 3, 2,\n",
       "       3, 2, 1, 3, 2, 2, 2, 0, 3, 0, 2, 0, 1, 1, 3, 4, 4, 2, 0, 3, 2, 0,\n",
       "       1, 0, 4, 4, 3, 1, 2, 1, 2, 0, 4, 0, 2, 2, 0, 1, 0, 1, 4, 0, 3, 4,\n",
       "       3, 0, 1, 1, 0, 4, 4, 2, 3, 2, 0, 1, 1, 4, 4, 0, 2, 3, 2, 3, 3, 2,\n",
       "       1, 0, 0, 1, 3, 2, 4, 1, 4, 4, 4, 2, 3, 3, 3, 3, 1, 2, 2, 0, 3, 0,\n",
       "       1, 1, 0, 1, 1, 4, 2, 3, 1, 2, 1, 1, 2, 2, 1, 3, 3, 3, 3, 2, 0, 1,\n",
       "       2, 3, 3, 3, 4, 3, 4, 3, 4, 1, 1, 0, 2, 1, 2, 0, 3, 0, 0, 4, 3, 4,\n",
       "       0, 0, 3, 2, 2, 4, 2, 1, 1, 2, 2, 2, 1, 4, 0, 0, 3, 3, 3, 2, 1, 0,\n",
       "       0, 2, 1, 0, 2, 3, 4, 1, 4, 0, 1, 4, 4, 3, 3, 1, 3, 0, 0, 1, 2, 3,\n",
       "       4, 4, 1, 2, 4, 1, 2, 1, 0, 2, 4, 4, 2, 1, 0, 4, 4, 3, 4, 2, 1, 0,\n",
       "       1, 2, 1, 3, 3, 0, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 0, 2, 0, 0, 1, 3,\n",
       "       0, 0, 4, 2, 4, 0, 3, 3, 0, 4, 4, 3, 4, 2, 2, 3, 0, 0, 2, 0, 1, 4,\n",
       "       2, 2, 0, 2, 3, 2, 0, 4, 4, 4, 4, 4, 0, 2, 0, 4, 0, 0, 0, 3, 0, 3,\n",
       "       0, 4, 0, 0, 0, 4, 4, 2, 2, 3, 0, 0, 1, 3, 3, 4, 1, 0, 0, 2, 2, 1,\n",
       "       3, 1, 2, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "labels_pred = model_dt.predict(features_test)\n",
    "display(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-ideal",
   "metadata": {},
   "source": [
    "### Simple Decision Tree Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "radical-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7955056179775281\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "elegant-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       102\n",
      "           1       0.84      0.75      0.79        77\n",
      "           2       0.73      0.79      0.75        84\n",
      "           3       0.88      0.89      0.88       102\n",
      "           4       0.81      0.80      0.81        80\n",
      "\n",
      "    accuracy                           0.80       445\n",
      "   macro avg       0.80      0.79      0.79       445\n",
      "weighted avg       0.80      0.80      0.80       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "report = classification_report(labels_test, labels_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-blocking",
   "metadata": {},
   "source": [
    "## Classification: Using Gaussian Naive Bayes Model (gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "million-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sunrise-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-amazon",
   "metadata": {},
   "source": [
    "**TODO: Do some hyperparameters fine-tuning steps here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "decreased-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_gnb.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "working-glory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3, 3, 0, 2, 3, 3, 1, 0, 2, 2, 4, 2, 3, 0, 2, 1, 3, 3, 3,\n",
       "       1, 0, 3, 1, 0, 0, 1, 0, 3, 0, 1, 4, 0, 0, 0, 0, 3, 1, 2, 3, 4, 0,\n",
       "       4, 0, 0, 1, 0, 1, 1, 2, 3, 1, 3, 3, 0, 4, 0, 4, 4, 3, 2, 3, 1, 3,\n",
       "       3, 4, 4, 1, 4, 0, 4, 4, 0, 3, 1, 2, 3, 0, 3, 2, 4, 4, 2, 3, 3, 0,\n",
       "       0, 2, 4, 2, 1, 0, 4, 4, 3, 3, 1, 0, 3, 3, 3, 2, 4, 0, 4, 2, 2, 3,\n",
       "       4, 3, 0, 1, 2, 4, 4, 0, 3, 3, 3, 0, 2, 0, 3, 1, 3, 3, 1, 2, 0, 4,\n",
       "       2, 4, 3, 2, 2, 0, 3, 2, 1, 2, 2, 1, 3, 0, 3, 3, 1, 4, 2, 0, 3, 2,\n",
       "       3, 2, 1, 3, 2, 2, 4, 1, 3, 0, 2, 0, 2, 1, 3, 4, 4, 2, 4, 3, 2, 0,\n",
       "       1, 0, 4, 4, 3, 1, 2, 4, 4, 0, 4, 0, 0, 2, 0, 0, 0, 4, 4, 0, 3, 4,\n",
       "       3, 0, 1, 1, 0, 1, 4, 0, 3, 2, 0, 1, 2, 4, 4, 0, 0, 3, 0, 3, 0, 2,\n",
       "       1, 0, 2, 1, 3, 2, 4, 1, 2, 4, 4, 0, 3, 3, 3, 3, 1, 2, 3, 0, 3, 2,\n",
       "       1, 1, 2, 0, 1, 4, 0, 3, 1, 2, 1, 1, 2, 2, 1, 3, 3, 3, 3, 2, 0, 2,\n",
       "       2, 0, 1, 3, 0, 4, 4, 3, 4, 1, 1, 0, 2, 0, 2, 0, 3, 0, 4, 4, 3, 4,\n",
       "       0, 0, 3, 2, 4, 0, 2, 1, 1, 2, 2, 2, 1, 4, 0, 0, 4, 3, 3, 2, 1, 0,\n",
       "       0, 2, 1, 2, 2, 3, 4, 1, 4, 0, 1, 4, 4, 3, 3, 3, 3, 3, 0, 1, 2, 3,\n",
       "       4, 4, 1, 0, 4, 4, 2, 1, 0, 2, 4, 4, 2, 1, 0, 4, 4, 3, 4, 1, 1, 2,\n",
       "       4, 2, 1, 3, 3, 4, 2, 2, 3, 3, 2, 2, 0, 2, 3, 4, 4, 2, 0, 0, 2, 3,\n",
       "       3, 0, 4, 2, 4, 0, 3, 3, 0, 4, 4, 3, 4, 2, 2, 0, 4, 1, 0, 2, 4, 4,\n",
       "       2, 3, 2, 0, 2, 0, 0, 4, 4, 1, 4, 3, 0, 2, 0, 4, 0, 0, 0, 3, 0, 3,\n",
       "       4, 2, 0, 0, 2, 4, 4, 2, 2, 3, 0, 0, 1, 3, 3, 4, 3, 2, 0, 2, 3, 4,\n",
       "       1, 1, 2, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "labels_pred = model_gnb.predict(features_test)\n",
    "display(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-dance",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afraid-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9213483146067416\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "joint-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       102\n",
      "           1       0.98      0.83      0.90        77\n",
      "           2       0.86      0.93      0.89        84\n",
      "           3       1.00      0.99      1.00       102\n",
      "           4       0.85      0.93      0.89        80\n",
      "\n",
      "    accuracy                           0.92       445\n",
      "   macro avg       0.92      0.92      0.92       445\n",
      "weighted avg       0.93      0.92      0.92       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "report = classification_report(labels_test, labels_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-piano",
   "metadata": {},
   "source": [
    "**TODO: Test other classification methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "suitable-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# For saving the model:\n",
    "# Check the sklearn.save\n",
    "# Search: How to persist a model sklearn\n",
    "# How to serve on "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-lighter",
   "metadata": {},
   "source": [
    "## Saving A Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-detective",
   "metadata": {},
   "source": [
    "Current Accuracy Scores on `test` data:\n",
    "\n",
    "- Random Forest -- No Tuning: 0.9461\n",
    "- Logistic Regression: 0.9730\n",
    "- K-Nearest Neighbors: 0.9663\n",
    "- Simple Decision Tree: 0.7978\n",
    "- Gaussian Naive Bayes: 0.9213"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-murray",
   "metadata": {},
   "source": [
    "Since our best accuracy model so far is Logistic Regression, let's export this model so we can use it in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-venue",
   "metadata": {},
   "source": [
    "### Using `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "european-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "billion-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the right directory\n",
    "pkl_filename = \"../saved-classification-models/bbc/plain-logistic-regression-pickle.pkl\"\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model_lr, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-norfolk",
   "metadata": {},
   "source": [
    "To later load and use this file into a new workspace, use the following steps:\n",
    "\n",
    "```python\n",
    "# Load the saved model from file\n",
    "with open('path/to/pkl_filename.pkl', 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "\n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(Xtest, Ytest)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "Ypredict = pickle_model.predict(Xtest)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-buffer",
   "metadata": {},
   "source": [
    "### Using `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "turkish-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "phantom-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the right directory\n",
    "pkl_filename = \"../saved-classification-models/bbc/plain-logistic-regression-joblib.pkl\"\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    joblib.dump(model_lr, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-privacy",
   "metadata": {},
   "source": [
    "To later load and use this file into a new workspace, use the following steps:\n",
    "\n",
    "```python\n",
    "# Load the saved model from file\n",
    "with open('path/to/pkl_filename.pkl', 'rb') as file:\n",
    "    joblib_model = joblib.load(file)\n",
    "\n",
    "# Calculate the accuracy score and predict target values\n",
    "score = joblib_model.score(Xtest, Ytest)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "Ypredict = joblib_model.predict(Xtest)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-safety",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-updating",
   "metadata": {},
   "source": [
    "### Using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-proceeding",
   "metadata": {},
   "source": [
    "Previously, our best score with Random Forest with no HPT was 0.9461"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-general",
   "metadata": {},
   "source": [
    "#### Using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "prepared-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "favorite-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the grid\n",
    "n_estimators = [100,300,500,800,1200]\n",
    "max_depth = [5,10,15,20,25,30]\n",
    "min_samples_split = [5,10,20,30,40,50,60,70,80,90,100]\n",
    "min_samples_leaf = [1,5,10,15]\n",
    "\n",
    "# Finalize the params for GridSearchCV\n",
    "hyper_params = dict(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "italian-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with GridSearchCV: 5-folds CV\n",
    "model_rf = RandomForestClassifier(random_state=777)\n",
    "grid_cv_rf = GridSearchCV(\n",
    "    model_rf, \n",
    "    hyper_params, \n",
    "    cv=3, # 3-folds-CV\n",
    "    verbose=4,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "color-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1320 candidates, totalling 3960 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-49b2bba0f558>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit on the training set to get the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_cv_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\default\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit on the training set to get the best model\n",
    "best_rf = grid_cv_rf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
